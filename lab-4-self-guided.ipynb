{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f20ccea4-1ef0-4f90-a0c3-3cbe1b0f7adc",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "## Data wrangling and transformation\n",
    "\n",
    "Often, datasets need to go through a series of data wrangling and transformation steps before they are ready for analysis or visualisation tasks. This lab will demonstrate several data wrangling and transformation operations for raster, vector, and tabular data. \n",
    "\n",
    "We will start with a subset of the AgriFieldNet Competition Dataset <a href=\"https://mlhub.earth/data/ref_agrifieldnet_competition_v1\" target=\"_blank\">(Radiant Earth Foundation and IDinsight, 2022)</a> which has been published to encourage people to develop machine learning models that classify a field's crop type from satellite images. This dataset consists of a series of directories with each directory corresponding to a 256 x 256 pixel image footprint. Inside each directory are the following files:\n",
    "\n",
    "* 12 GeoTIFF files corresponding to spectral reflectance in different wavelengths from Sentinel-2 data. \n",
    "* 1 GeoTIFF file with non-zero pixels corresponding to a crop type label. \n",
    "* 1 GeoTIFF file with non-zero pixels corresponding to a field id. \n",
    "* 1 JSON metadata file. \n",
    "\n",
    "This data is subset from a larger dataset covering agricultural fields in four Indian states: Odisha, Uttar Pradesh, Bihar, and Rajasthan. The field boundaries and crop type labels were captured by data collectors from IDinsight's Data on Demand team and the satellite image preparation was undertaken by the Radiant Earth Foundation. \n",
    "\n",
    "### Task\n",
    "\n",
    "Our task is to combine all the raster data in a folder into a tabular dataset that can be used for machine learning tasks to predict a field's crop type. Specifically, we will  transform a collection of GeoTiff files into a tabular dataset with columns for each field id, crop type, and field average spectral reflectance values. We will also store geometry data representing the location of each field in a geometry column. \n",
    "\n",
    "![](https://github.com/data-analysis-3300-3003/figs/raw/main/week-4-overview.jpg)\n",
    "\n",
    "You will learn a range of common data transformation operations to wrangle datasets into a structure suitable for analysis and visualisation.   \n",
    "\n",
    "**This lab will focus on transformation operations applied to tabular and vector data.** This lab will cover:\n",
    "\n",
    "* **attribute operations:** data cleaning refresher (from week 3).\n",
    "* **attribute operations:** subsetting `DataFrame`s based on conditions.\n",
    "* **attribute operations:** appending (concatenating) rows to tabular `DataFrame` objects.\n",
    "* **attribute operations:** group-by and summarise operations of tabular `DataFrame` objects.\n",
    "* **attribute operations:** key-based relational joins between two tables.\n",
    "* **raster-vector operations:** vectorising raster data.\n",
    "* **geometry operations:** computing polygon geometry centroids.\n",
    "* **attribute operations:** key-based relational joins between two tables.\n",
    "* **spatial data operations:** spatial joins of two `GeoDataFrame` objects. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f2b402-819a-47b4-a1d2-0d0e2209a220",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Run the labs\n",
    "\n",
    "You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. **If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.**\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/data-analysis-3300-3003/colab/blob/main/lab-4-self-guided.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "### Download data\n",
    "\n",
    "If you need to download the data for this lab, run the following code snippet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6417f87-ebf2-4955-b593-5e2ccc7748ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"week-4\" not in os.listdir(os.getcwd()):\n",
    "    os.system('wget \"https://github.com/data-analysis-3300-3003/data/raw/main/data/week-4.zip\"')\n",
    "    os.system('unzip \"week-4.zip\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c646b92f-1966-4904-b221-f4715630f341",
   "metadata": {},
   "source": [
    "### Working in Colab\n",
    "\n",
    "If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e478e4-615a-4713-bae1-75cd440e48b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install geopandas\n",
    "    !pip install pyarrow\n",
    "    !pip install mapclassify\n",
    "    !pip install rasterio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf742d0-c865-4407-9551-b35cd21a6774",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e0488-dedf-4e03-9cb3-363cffec6b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import plotly.io as pio\n",
    "import shapely.geometry\n",
    "import pprint\n",
    "\n",
    "from rasterio import features\n",
    "\n",
    "# setup renderer\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    pio.renderers.default = \"colab\"\n",
    "else:\n",
    "    pio.renderers.default = \"jupyterlab\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283743b5-18c6-4260-8569-a8e838bc932b",
   "metadata": {},
   "source": [
    "### Preliminary processing\n",
    "\n",
    "This weeks self-guided lab will pick up from lab-4 where we'd created a program to:\n",
    "\n",
    "1. read GeoTIFF files into NumPy `ndarray` objects\n",
    "2. stack the `ndarray` objects to create a multiband raster representation of the data\n",
    "3. reshape the multiband `ndarray` objects to a tabular-like structure\n",
    "\n",
    "In this lab we will extend this program by converting the `ndarray` representation of a table to `DataFrame` object which we will further process with a range of tabular attribute and vector operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d62be97-a8fe-41f5-a600-5d4108c45164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to data\n",
    "image_dir_path = os.path.join(os.getcwd(), \"week-4\", \"images\", \"ref_agrifieldnet_competition_v1_source_0a664\")\n",
    "\n",
    "# stacking bands \n",
    "\n",
    "# Sentinel-2 band names \n",
    "s2_bands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "\n",
    "# empty list to append ndarray of reflectance value for each band to\n",
    "bands = []\n",
    "    \n",
    "# loop over each band, read in the data from the corresponding GeoTIFF file into an ndarray\n",
    "for b in s2_bands:\n",
    "    print(f\"reading {b}.tif\")\n",
    "    band_path = os.path.join(image_dir_path, b + \".tif\")\n",
    "    with rasterio.open(band_path) as src:\n",
    "        # append the ndarray storing the Sentinel-2 reflectance data for a band to a list\n",
    "        bands.append(src.read(1))\n",
    "\n",
    "# stack all bands in the list to create a multiband raster\n",
    "multiband_raster = np.stack(bands)\n",
    "    \n",
    "# make NDVI band\n",
    "red = multiband_raster[3,:,:].astype(\"float64\")\n",
    "nir = multiband_raster[7,:,:].astype(\"float64\")\n",
    "ndvi = (nir-red)/(nir+red)\n",
    "ndvi = np.expand_dims(ndvi, axis=0) # add a bands axis\n",
    "multiband_raster = np.concatenate((multiband_raster, ndvi), axis=0) # stack the ndvi band\n",
    "    \n",
    "### HERE WE ARE STACKING THE FIELD ID BAND \n",
    "field_id_path = os.path.join(image_dir_path, \"field/field_ids.tif\")\n",
    "with rasterio.open(field_id_path) as src:\n",
    "    field_ids = src.read().astype(\"float64\")\n",
    "    field_ids[field_ids == 0] = np.nan\n",
    "    multiband_raster = np.concatenate((multiband_raster, field_ids), axis=0)\n",
    "    \n",
    "## HERE WE ARE STACKING THE CROP TYPE LABELS BAND \n",
    "labels_path = os.path.join(image_dir_path, \"label/raster_labels.tif\")\n",
    "with rasterio.open(labels_path) as src:\n",
    "    multiband_raster = np.concatenate((multiband_raster, src.read()), axis=0)\n",
    "\n",
    "# reshape multiband raster to tabular format\n",
    "rows = multiband_raster.shape[1]\n",
    "cols = multiband_raster.shape[2]\n",
    "n_bands = multiband_raster.shape[0]\n",
    "reshaped = multiband_raster.reshape(n_bands, rows*cols)\n",
    "tabular = reshaped.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f765e423-0679-41ec-bb91-cfed92890d42",
   "metadata": {},
   "source": [
    "## Attribute data operations\n",
    "\n",
    "### Pandas `DataFrame` and data cleaning\n",
    "\n",
    "As our data is now in a tabular structure it makes sense to convert it from a NumPy `ndarray` object to Pandas a `DataFrame` object. Pandas `DataFrame` objects, and the pandas package more generally, are based on NumPy but have been tailored for working with tabular datasets. For example, a NumPy `ndarray` stores data of the same type in an array-like structure (e.g. all elements are integers). A pandas `DataFrame` can store different type data in different columns (e.g. column 0 is string, column 1 is floating point, etc.), but the values within each column are the same type and each column is typically a `PandasArray` which is based on a NumPy `ndarray`. \n",
    "\n",
    "Columns in a pandas `DataFrame` are called `Series` and a `Series` can be objects in your program independent of a `DataFrame`. A `Series` is an array-like sequence of values stored in a `PandasArray` which wraps a NumPy `ndarray`, and a `DataFrame` creates a tabular structure by combining one or more `Series`. \n",
    "\n",
    "Operations on Pandas `DataFrame`s also borrow from NumPy's style such as avoiding for-loops; however, they also provide several features and functions geared towards working with tabular datasets. A selection of these features include:\n",
    "\n",
    "* indexing using column names\n",
    "* relational database style operations including key-based joins, conditional filtering and selection of data, and group-by and summarise\n",
    "* support for working with time-series\n",
    "* more tools for handling missing data\n",
    "\n",
    "Thus, Pandas `DataFrame` objects are useful for many attribute data transorfmation operations.\n",
    "\n",
    "To convert a NumPy `ndarray` to a Pandas `DataFrame` we pass the array into the <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame\" target=\"_blank\">`DataFrame`'s constructor</a> function helpfully named `DataFrame()`. The constructor function expects the NumPy `ndarray` and a list of column labels as arguments. \n",
    "\n",
    "Let's quickly recap our data structures after we have performed several raster transformation operations to the GeoTIFF files. We have an `ndarray` object, `tabular`, which is a 2-Dimensional NumPy `ndarray` representing 256 x 256 pixel images in a tabular structure with pixels aligned down the rows (0-axis) and bands aligned along the columns (1-axis). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb29afce-2d0a-492d-b759-ddc79fd23b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"tabular is of type {type(tabular)}\")\n",
    "print(f\"tabular is an ndarray with shape {tabular.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2a9af6-4d0f-40de-9bad-bc6a6814ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ndarray to Pandas DataFrame\n",
    "s2_bands = ['B01', 'B02', 'B03', 'B04','B05', 'B06', 'B07', 'B08','B8A', 'B09', 'B11', 'B12']\n",
    "\n",
    "# create a DataFrame object from the first element in tables\n",
    "tmp_df = pd.DataFrame(tabular, columns=s2_bands + [\"ndvi\", \"field_id\", \"labels\"])\n",
    "tmp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9766a602-3355-46dc-8203-b2e12b50c41e",
   "metadata": {},
   "source": [
    "Looking at the `DataFrame` we can clearly see some `NaN` values in the `field_id` column. As each row in this `DataFrame` represents a pixel in the image, rows with `NaN` values in the `field_id` column are pixels which don't have a crop type label. Therefore, we can drop them using the <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html\" target=\"_blank\">`dropna()` method</a> - this will drop all rows from the `DataFrame` where there is a `NaN` value.\n",
    "\n",
    "Let's inspect some metadata for the `DataFrame` we have created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2051a-13e8-42b1-94bc-98c7781886a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1338fd0c-2ad9-479e-bb8c-50659739ef30",
   "metadata": {},
   "source": [
    "The `DataFrame`'s `info()` method returns a summary of the column's data types, count of non-null data, and the memory usage for the object. We can see that the `field_id` and `labels` columns are float64; however, these columns are storing categorical data so integer numbers would be a more appropriate type. Therefore, we can use the `astype()` method to convert these columns to integer type. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b4201-c24e-4f45-8a34-7561ee46ada3",
   "metadata": {},
   "source": [
    "#### Recap quiz\n",
    "\n",
    "**Can you use the `dropna()` and `astype()` methods to i) drop all rows with `NaN` values, and ii) convert the `field_id` and `labels` columns to `int32` type? Use the pandas docs for example uses of the `dropna()` and `astype()` methods.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e3c960-8d44-4d26-b70c-e3e3afc20673",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c36ac76-bc55-4475-9a0f-bde0d8065d5e",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```python\n",
    "tmp_df = tmp_df.dropna()\n",
    "tmp_df = tmp_df.astype({\"field_id\": \"int32\", \"labels\": \"int32\"})\n",
    "# Check the cast to integer type has worked. \n",
    "# Also, note the reduced memory usage after dropping all the nan rows\n",
    "tmp_df.info()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ac0b10-6b93-4675-9a85-e0b1855607c5",
   "metadata": {},
   "source": [
    "### Grouped summaries\n",
    "\n",
    "Another common attribute operation when working with tabular data is performing grouped aggregations and summaries. For example, often we want to compute the mean, median, min, max, or sum of data values within groups in our dataset. This could be to generate summary tables for reporting purposes or an intermediate step in a data transformation workflow. \n",
    "\n",
    "Our goal for this data transformation workflow is to generate average spectral reflectance values in different wavelengths from Sentinel-2 images for each field with a crop type label. So far we have created a table where each row represents one pixel in a 256 x 256 image footprint and we have many pixels per field. We need to compute the average reflectance values for each field. This is a group-by and summarise operation - grouping by field and summarising using the mean. \n",
    "\n",
    "A group-by and summarise operation can be conceptualised as a sequence of split-apply-combine steps <a href=\"https://wesmckinney.com/book/data-aggregation.html#groupby_fundamentals\" target=\"_blank\">McKinney (2022)</a>:\n",
    "\n",
    "* **Split** your dataset into groups.\n",
    "* **Apply** a function to values in each group as a summary.\n",
    "* **Combine** the results of applying the function to each group. \n",
    "\n",
    "For our dataset we need to group-by `field_id` and `labels` (crop type column) and compute the mean of spectral reflectance values within each group. \n",
    "\n",
    "Pandas `DataFrame`s have a `groupby()` method that can take in a list of one or more column names. Calling this method returns a `GroupBy` object that creates groups from your dataset for each of the unique values of the grouping columns and can be used to apply summary operations to each group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d976db-2bc8-4cab-a41c-4c10ce8998c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a group using field id and crop type\n",
    "tmp_df_groups = tmp_df.groupby([\"field_id\", \"labels\"])\n",
    "print(tmp_df_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548e1e45-35fd-41e8-a135-ef91ae6f245a",
   "metadata": {},
   "source": [
    "Finally, we need to apply our summary operations to each group. We can do this by calling a function on the `GroupBy` object. A useful function for data exploration tasks is calling `size()` which tells us the number of observations in each group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630615e-f8c8-4ea4-ba63-9fefe5cfe470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of groups\n",
    "tmp_df_groups.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500e5fc2-e8e0-4df2-aeff-9be7cfa31285",
   "metadata": {},
   "source": [
    "By calling `size()` on the `GroupBy` object we can see that we have a group with a `field_id` of `81` and a crop type label of `1` (wheat). There are 70 observations in this group. \n",
    "\n",
    "If we want to compute the mean of the spectral reflectance values within each group we can call `mean()` instead of `size()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e78b80-c7b7-4c7a-b255-ac2085f61529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean spectral reflectance values per group\n",
    "tmp_df_groups.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca9b1a8-c390-43ec-88e4-3bbbbf344883",
   "metadata": {},
   "source": [
    "We're now in a position to update our data transformation routine to include converting the `ndarray` object in a tabular-like structure to Pandas `DataFrames`, data cleaning to drop `NaN` pixels, and computing the mean spectral reflectance values for each `field_id` and crop type `label` combination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce734806-1d34-432a-bcf0-602a28faf9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to data\n",
    "image_dir_path = os.path.join(os.getcwd(), \"week-4\", \"images\", \"ref_agrifieldnet_competition_v1_source_0a664\")\n",
    "\n",
    "# stacking bands \n",
    "\n",
    "# Sentinel-2 band names \n",
    "s2_bands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "\n",
    "# empty list to append ndarray of reflectance value for each band to\n",
    "bands = []\n",
    "    \n",
    "# loop over each band, read in the data from the corresponding GeoTIFF file into an ndarray\n",
    "for b in s2_bands:\n",
    "    print(f\"reading {b}.tif\")\n",
    "    band_path = os.path.join(image_dir_path, b + \".tif\")\n",
    "    with rasterio.open(band_path) as src:\n",
    "        # append the ndarray storing the Sentinel-2 reflectance data for a band to a list\n",
    "        bands.append(src.read(1))\n",
    "\n",
    "# stack all bands in the list to create a multiband raster\n",
    "multiband_raster = np.stack(bands)\n",
    "    \n",
    "# make NDVI band\n",
    "red = multiband_raster[3,:,:].astype(\"float64\")\n",
    "nir = multiband_raster[7,:,:].astype(\"float64\")\n",
    "ndvi = (nir-red)/(nir+red)\n",
    "ndvi = np.expand_dims(ndvi, axis=0) # add a bands axis\n",
    "multiband_raster = np.concatenate((multiband_raster, ndvi), axis=0) # stack the ndvi band\n",
    "    \n",
    "### HERE WE ARE STACKING THE FIELD ID BAND \n",
    "field_id_path = os.path.join(image_dir_path, \"field/field_ids.tif\")\n",
    "with rasterio.open(field_id_path) as src:\n",
    "    field_ids = src.read().astype(\"float64\")\n",
    "    field_ids[field_ids == 0] = np.nan\n",
    "    multiband_raster = np.concatenate((multiband_raster, field_ids), axis=0)\n",
    "    \n",
    "## HERE WE ARE STACKING THE CROP TYPE LABELS BAND \n",
    "labels_path = os.path.join(image_dir_path, \"label/raster_labels.tif\")\n",
    "with rasterio.open(labels_path) as src:\n",
    "    multiband_raster = np.concatenate((multiband_raster, src.read()), axis=0)\n",
    "\n",
    "# reshape multiband raster to tabular format\n",
    "rows = multiband_raster.shape[1]\n",
    "cols = multiband_raster.shape[2]\n",
    "n_bands = multiband_raster.shape[0]\n",
    "reshaped = multiband_raster.reshape(n_bands, rows*cols)\n",
    "tabular = reshaped.T\n",
    "    \n",
    "### HERE WE CONVERT TO DATAFRAMES AND DROP NAN VALUES\n",
    "tmp_df = pd.DataFrame(tabular, columns=s2_bands + [\"ndvi\", \"field_id\", \"labels\"])\n",
    "dfs = tmp_df.dropna()\n",
    "    \n",
    "dfs = dfs.astype({\"field_id\": \"int32\", \"labels\": \"int32\"})\n",
    "dfs = dfs.groupby([\"field_id\", \"labels\"]).mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb87db6-9156-4328-b50a-c77e17b4f088",
   "metadata": {},
   "source": [
    "Let's quickly inspect the output. We should have one row per `field_id` and crop type `label` group. The `DataFrame` storing the results of this routine are referenced by the variable `dfs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac2b07-625e-490a-96a0-90fc547b0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ec3fe9-77c1-406a-9d3a-1c5772bf14eb",
   "metadata": {},
   "source": [
    "## Raster-vector operations and vector operations\n",
    "\n",
    "We're almost at the stage where we've processed a number of GeoTIFF files stored across many directories into a tabular dataset in a `DataFrame` object ready for machine learning. However, there are two more columns we need to create and append to the `DataFrame`. The first is a `geometry` column recording the centroid of each field. This allows us to keep a record of each field's geographic location. We'll also use this centroid to identify the district (an administrative boundary below the State-level in India) each field is located in.  \n",
    "\n",
    "To compute the centroid for each field we need to perform some raster-vector operations where each raster dataset is converted to a vector dataset. This is called vectorisation and can be achieved using `rasterio`'s <a href=\"https://rasterio.readthedocs.io/en/latest/api/rasterio.features.html#rasterio.features.shapes\" target=\"_blank\">`shapes()` function</a> which returns the shape and value of connected regions in a raster dataset. Pixels belonging the same field in a raster layer should be connected (i.e. their edges touch) and they should have the same value (field id). Thus, applying the `shapes()` function to the raster layer of field ids should return vector polygons for each field outline.\n",
    "\n",
    "To do this we'll need to use the `field_ids.tif` file. Let's quickly inspect this files again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a08053f-a384-4f9a-8d62-dda18ad92ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_id_path = os.path.join(image_dir_path, \"field/field_ids.tif\")\n",
    "with rasterio.open(field_id_path) as src:\n",
    "    print(f\"Printing metadata for field_ids.tif\")\n",
    "    pprint.pprint(src.meta)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bad732-e6e1-4a6b-bbae-06a32c331753",
   "metadata": {},
   "source": [
    "We have printed a dictionary objects of metadata for the `field_ids.tif` file. \n",
    "\n",
    "Let's look at what the `shapes()` function returns for `field_ids.tif`. \n",
    "\n",
    "The `shapes()` function takes in a NumPy `ndarray` of raster values (generated by `src.read()` which reads the raster values from the GeoTIFF file into a NumPy `ndarray` in memory) and returns a generator object which generates a tuple for each shape in the raster data. The first element of the tuple is a dictionary object of coordinates and the type of geometry (e.g. point, line, polygon). The second element of the tuple is the attribute value that corresponds to the geometry. We can convert the generator into a list of tuples. \n",
    "\n",
    "![](https://github.com/data-analysis-3300-3003/figs/raw/main/week-4-raster-to-vector.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b041d250-642e-47f7-8197-5b7397ffa18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_id_path = os.path.join(image_dir_path, \"field/field_ids.tif\")\n",
    "with rasterio.open(field_id_path) as src:\n",
    "    # shapes is a generator\n",
    "    shapes = features.shapes(src.read(), transform=src.transform)\n",
    "\n",
    "    # list of geometry and shape value\n",
    "    field_shapes = list(shapes)\n",
    "    \n",
    "    # pretty print the first two elements of field_shapes\n",
    "    pprint.pprint(field_shapes[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31020072-feaf-441a-a271-b2dc5b274b26",
   "metadata": {},
   "source": [
    "At this stage, we've converted our raster data to a list of numbers representing coordinates for the shape. We now need to turn this list of coordinates into a geometry object. In Python, geometries are represented as <a href=\"https://shapely.readthedocs.io/en/stable/geometry.html\" target=\"_blank\">Shapely</a> `Geometry` objects. The `geometry` column in a GeoPandas `GeoDataFrame` is a `Series` of Shapely `Geometry` objects.\n",
    "\n",
    "To create a <a href=\"https://shapely.readthedocs.io/en/stable/geometry.html\" target=\"_blank\">Shapely</a> `Geometry` object we extract the list of coordinates and pass them into the `shapely.geometry.shape()` function. \n",
    "\n",
    "Printing `geom` should return a list of Shapely `Geometry` objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31193e75-d473-45ee-ab50-cd8b8edd54df",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_id_path = os.path.join(image_dir_path, \"field/field_ids.tif\")\n",
    "with rasterio.open(field_id_path) as src:\n",
    "    # shapes is a generator\n",
    "    shapes = features.shapes(src.read(), transform=src.transform)\n",
    "\n",
    "    # list of geometry and shape value\n",
    "    field_shapes = list(shapes)\n",
    "\n",
    "    # create a list of Shapely Geometry objects\n",
    "    geom = []\n",
    "    for s in field_shapes:\n",
    "        geom.append(shapely.geometry.shape(s[0]))\n",
    "\n",
    "    print(geom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb31464a-d1c4-4e97-9405-f17d52eb5aa1",
   "metadata": {},
   "source": [
    "We can also plot an element of `geom` to show it is a Shapely `Geometry` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedbf951-00c8-4c0b-94d4-60adcf193898",
   "metadata": {},
   "outputs": [],
   "source": [
    "geom[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8748e6f6-4d77-4e3c-a969-f94a063f2dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "geom[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd03bcb-46d3-4e60-a598-c30669a3191e",
   "metadata": {},
   "source": [
    "#### Recap quiz\n",
    "\n",
    "<details>\n",
    "    <summary><b>What object are we creating with <code>[]</code>?</b></summary>\n",
    "An empty list object.\n",
    "</details>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<details>\n",
    "    <summary><b>What type of object is <code>geom</code> and what elements does it store?</b></summary>\n",
    "It is a list object which is storing a list of Shapely <code>Geometry</code> objects.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b89daf-83e0-4d01-84b1-8a045f3455da",
   "metadata": {},
   "source": [
    "Next, we compute the centroid for the polygon shape of the field. Computing a centroid is a geometry operation where the shape's geometry is converted from a polygon to a point feature. To efficiently compute the centroid for the shapes returned by `shapes()` we can convert the list of `Geometry` objects to a `GeoSeries` and then use the <a href=\"https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.centroid.html\" target=\"_blank\">`GeoSeries` `centroid` attribute</a> to return a `GeoSeries` of centroids. \n",
    "\n",
    "Inspecting this `GeoSeries` should reveal a sequence of point `Geometry` objects have been computed. This `GeoSeries` object is now referenced by `geom`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0ce8ed-32ba-4108-9348-73c3f689eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_id_path = os.path.join(image_dir_path, \"field/field_ids.tif\")\n",
    "with rasterio.open(field_id_path) as src:\n",
    "    # shapes is a generator\n",
    "    shapes = features.shapes(src.read(), transform=src.transform)\n",
    "\n",
    "    # list of geometry and shape value\n",
    "    field_shapes = list(shapes)\n",
    "\n",
    "    # create a list of Shapely Geometry objects\n",
    "    geom = []\n",
    "    for s in field_shapes:\n",
    "        geom.append(shapely.geometry.shape(s[0]))\n",
    "\n",
    "    # compute centroids\n",
    "    geom = gpd.GeoSeries(geom, crs=src.crs).centroid\n",
    "\n",
    "    print(geom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed9839-cf8b-424d-8f0f-4c3fa43e81f8",
   "metadata": {},
   "source": [
    "Now we have computed the centroid for each field, we can convert the points to a common coordinate reference system (`EPSG:4326`) using GeoPandas `to_crs()` method. We convert the points to a new coordinate system, `EPSG:4326` which uses latitude and longitude values, to be able to perform vector operations using two vector datasets in future tasks (it is important that vector datasets have the same coordinate reference system to get correct and intended results). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a5987a-df00-4615-bbc5-8862aa1b4478",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_id_path = os.path.join(image_dir_path, \"field/field_ids.tif\")\n",
    "with rasterio.open(field_id_path) as src:\n",
    "    # shapes is a generator\n",
    "    shapes = features.shapes(src.read(), transform=src.transform)\n",
    "\n",
    "    # list of geometry and shape value\n",
    "    field_shapes = list(shapes)\n",
    "\n",
    "    # create a list of Shapely Geometry objects\n",
    "    geom = []\n",
    "    for s in field_shapes:\n",
    "        geom.append(shapely.geometry.shape(s[0]))\n",
    "\n",
    "    # compute centroids\n",
    "    geom = gpd.GeoSeries(geom, crs=src.crs).centroid\n",
    "\n",
    "    # reproject to EPSG 4326\n",
    "    geom = geom.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b44d02-0b33-4779-8264-2990dfd6d5ac",
   "metadata": {},
   "source": [
    "If we look at the object returned by the `features.shapes()`, it's a tuple with coordinates for connected raster pixels with the same value in the first element and the second element is the value of those raster pixels. \n",
    "\n",
    "```\n",
    "({'coordinates': [[(625350.0, 3010380.0),\n",
    "                    (625370.0, 3010380.0),\n",
    "                    (625370.0, 3010370.0),\n",
    "                    (625390.0, 3010370.0),\n",
    "                    (625390.0, 3010360.0),\n",
    "                    (625410.0, 3010360.0),\n",
    "                    (625410.0, 3010350.0),\n",
    "                    (625430.0, 3010350.0),\n",
    "                    (625430.0, 3010320.0),\n",
    "                    (625420.0, 3010320.0),\n",
    "                    (625420.0, 3010300.0),\n",
    "                    (625410.0, 3010300.0),\n",
    "                    (625410.0, 3010280.0),\n",
    "                    (625400.0, 3010280.0),\n",
    "                    (625400.0, 3010270.0),\n",
    "                    (625390.0, 3010270.0),\n",
    "                    (625390.0, 3010250.0),\n",
    "                    (625380.0, 3010250.0),\n",
    "                    (625380.0, 3010230.0),\n",
    "                    (625370.0, 3010230.0),\n",
    "                    (625370.0, 3010220.0),\n",
    "                    (625360.0, 3010220.0),\n",
    "                    (625360.0, 3010200.0),\n",
    "                    (625350.0, 3010200.0),\n",
    "                    (625350.0, 3010180.0),\n",
    "                    (625310.0, 3010180.0),\n",
    "                    (625310.0, 3010190.0),\n",
    "                    (625290.0, 3010190.0),\n",
    "                    (625290.0, 3010200.0),\n",
    "                    (625270.0, 3010200.0),\n",
    "                    (625270.0, 3010240.0),\n",
    "                    (625280.0, 3010240.0),\n",
    "                    (625280.0, 3010250.0),\n",
    "                    (625290.0, 3010250.0),\n",
    "                    (625290.0, 3010270.0),\n",
    "                    (625300.0, 3010270.0),\n",
    "                    (625300.0, 3010290.0),\n",
    "                    (625310.0, 3010290.0),\n",
    "                    (625310.0, 3010310.0),\n",
    "                    (625320.0, 3010310.0),\n",
    "                    (625320.0, 3010330.0),\n",
    "                    (625330.0, 3010330.0),\n",
    "                    (625330.0, 3010350.0),\n",
    "                    (625340.0, 3010350.0),\n",
    "                    (625340.0, 3010360.0),\n",
    "                    (625350.0, 3010360.0),\n",
    "                    (625350.0, 3010380.0)]],\n",
    "   'type': 'Polygon'},\n",
    "  1316.0)\n",
    "```\n",
    "\n",
    "#### Recap quiz\n",
    "\n",
    "This is a challenging quiz question, have a go at each of the steps and follow the pointers to previous code snippets or docs before reviewing the answer. \n",
    "\n",
    "**1. We need create another `Series` of field ids using the value of raster pixels corresponding to the coordinates in a tuple. We can do this by looping over `field_shapes` (the list of tuples) and accessing the element at index position one in the tuple.** *Have a look at how we looped over `field_shapes` and accessed the coordinates at index position 0 in `s` and appended the object to the list `geom`. Use this logic as a template for how you could access the value in index position 1 and append it to a list.*  \n",
    "\n",
    "**2. Once you have created this `Series`, you should set its type to `int32` using the `astype()` method.** *These are the <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html\" target=\"_blank\">docs</a> for `astype()`.* \n",
    "\n",
    "**3. Then we need to combine the `Series` of field ids with the `GeoSeries` of field centroids in a `GeoDataFrame`. You can do this using the `GeoDataFrame()` constructor function: `field_id_df = gpd.GeoDataFrame({'field_id': field_ids, 'geometry': geom})` (`field_ids` is a `Series` of field id values and `geom` is a `GeoSeries` of points.**\n",
    "\n",
    "**4. Finally, we need to drop rows in the `GeoDataFrame` with 0 values in the `field_id` column as 0 as these shapes don't have a crop type label.** *You can use the `loc[]` method for this and refer back to the **Subsetting pandas `DataFrame`s** section from the self-guided lab in week 3*.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea356e3-7378-4cbc-862c-4f916280b585",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_id_path = os.path.join(image_dir_path, \"field/field_ids.tif\")\n",
    "with rasterio.open(field_id_path) as src:\n",
    "    # shapes is a generator\n",
    "    shapes = features.shapes(src.read(), transform=src.transform)\n",
    "\n",
    "    # list of geometry and shape value\n",
    "    field_shapes = list(shapes)\n",
    "\n",
    "    # create a list of Shapely Geometry objects\n",
    "    geom = []\n",
    "    for s in field_shapes:\n",
    "        geom.append(shapely.geometry.shape(s[0]))\n",
    "\n",
    "    # compute centroids\n",
    "    geom = gpd.GeoSeries(geom, crs=src.crs).centroid\n",
    "\n",
    "    # reproject to EPSG 4326\n",
    "    geom = geom.to_crs(\"EPSG:4326\")\n",
    "        \n",
    "    ## ADD CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2ebea9-c5ae-4e99-b11b-d9cb9e896230",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "    \n",
    "```python\n",
    "field_id_path = os.path.join(image_dir_path, \"field/field_ids.tif\")\n",
    "with rasterio.open(field_id_path) as src:\n",
    "    # shapes is a generator\n",
    "    shapes = features.shapes(src.read(), transform=src.transform)\n",
    "\n",
    "    # list of geometry and shape value\n",
    "    field_shapes = list(shapes)\n",
    "\n",
    "    # create a list of Shapely Geometry objects\n",
    "    geom = []\n",
    "    for s in field_shapes:\n",
    "        geom.append(shapely.geometry.shape(s[0]))\n",
    "\n",
    "    # compute centroids\n",
    "    geom = gpd.GeoSeries(geom, crs=src.crs).centroid\n",
    "\n",
    "    # reproject to EPSG 4326\n",
    "    geom = geom.to_crs(\"EPSG:4326\")\n",
    "\n",
    "    # create a Series of field ids\n",
    "    field_ids = []\n",
    "    for f in field_shapes:\n",
    "        field_ids.append(f[1])\n",
    "\n",
    "    field_ids = pd.Series(field_ids).astype(\"int32\")\n",
    "\n",
    "    # Combine the Series and GeoSeries into a DataFrame\n",
    "    field_id_df = gpd.GeoDataFrame({'field_id': field_ids, 'geometry': geom})\n",
    "\n",
    "    # drop shapes with value 0\n",
    "    field_id_df = field_id_df.loc[field_id_df[\"field_id\"] > 0, :]\n",
    "\n",
    "    print(field_id_df)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e4c537-d6ee-4a89-a176-3dfdd30f68dd",
   "metadata": {},
   "source": [
    "If you've successfully completed the recap quiz, you should have a variable `field_id_df` that references a `GeoDataFrame` with a column of `field_id` values and a `geometry` column of field centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679f6d11-0364-47e9-a22d-c6c054b25d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_id_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a490b7-6b2e-4e73-a0e3-409d087da259",
   "metadata": {},
   "source": [
    "## Joins\n",
    "\n",
    "### Key-based joins\n",
    "\n",
    "We now have two separate data objects in our Python program. We have a `DataFrame` storing average spectral reflectance values for each field, field id, and crop type label attributes (this is referenced by the variable `dfs`). We also have a `GeoDataFrame` storing the field id attribute and the field centroid as a point `Geometry` (this is referenced by the variable `field_id_df`). \n",
    "\n",
    "When two tables have a matching column(s) we can use join operations to combine them. Rows in both tables are matched using common values in the matching column(s) and the joined table has columns from both tables. \n",
    "\n",
    "Joining tables is a common operation in relational databases using SQL and the same operations can be implemented in Pandas using <a href=\"https://pandas.pydata.org/docs/user_guide/merging.html#database-style-dataframe-or-named-series-joining-merging\" target=\"_blank\">`merge()`</a> functions. \n",
    "\n",
    "Some important concepts for join operations:\n",
    "\n",
    "* The columns with values used to match rows are called often called **keys**.\n",
    "* **one-to-one** joins are where there is exactly one match between rows in the two tables being joined.\n",
    "* **many-to-one** joins are where a row in one table can match one or more rows in another table.\n",
    "* **left joins** keep all rows in the left table and only matching rows in the right table. \n",
    "* **inner joins** keep only matching rows in the left and right tables. \n",
    "\n",
    "\n",
    "The Pandas <a href=\"https://pandas.pydata.org/docs/user_guide/merging.html#database-style-dataframe-or-named-series-joining-merging\" target=\"_blank\">`merge()`</a> docs and <a href=\"https://wesmckinney.com/book/data-wrangling.html#prep_merge_join\" target=\"_blank\">McKinney (2022)</a> provide useful explanations for how join operations work.\n",
    "\n",
    "![](https://github.com/data-analysis-3300-3003/figs/raw/main/week-4-joins.jpg)\n",
    "\n",
    "Let's use consider these concepts in the context of joining our `DataFrame` `dfs` storing average spectral reflectance values and crop type labels and our `GeoDataFrame` `field_id_df` which stores the field centroids. \n",
    "\n",
    "The matching column in both tables is `field_id`. This the joining key. \n",
    "\n",
    "We are joining the two tables on `field_id` which should be unique to each field. Therefore, we are implementing a one-to-one join. \n",
    "\n",
    "As we're using the field centroids for subsequent operations, we only want to keep fields that have a centroid value. Therefore, we'll use an inner join.\n",
    "\n",
    "Pandas `merge()` function expects the following arguments:\n",
    "\n",
    "* `left` - left table in the join.\n",
    "* `right` - right table in the join.\n",
    "* `how` - whether to use a left or inner join.\n",
    "* `left_on` - columns in left table to use as keys.\n",
    "* `right_on` - columns in the right table to use as keys.\n",
    "\n",
    "#### Recap quiz\n",
    "\n",
    "**Can you use the `merge()` function to perform an inner join using the `field_id` column combining `dfs` and `field_id_df`? If the join is successful you should see a `geometry` column appended to the columns in `dfs`. Assign the result of this `merge()` to the variable `joined_df`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce0b6ca-4bd3-4280-958a-41916a3b241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b703a70-a92e-4dfd-8d47-27c6bd5bbb6c",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```python\n",
    "joined_df = pd.merge(left=dfs, right=field_id_df, how=\"inner\", left_on=[\"field_id\"], right_on=[\"field_id\"])\n",
    "# display on the first few rows\n",
    "joined_df.head()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b714012-0809-4438-a364-3cfe629e4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert joined_df to GeoDataFrame\n",
    "joined_df = gpd.GeoDataFrame(joined_df, geometry=joined_df.geometry, crs=\"EPSG:4326\")\n",
    "type(joined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d688e82-9f0a-4d5e-b42e-0b8991d675d6",
   "metadata": {},
   "source": [
    "### Spatial Joins\n",
    "\n",
    "Spatial join operations join the attributes of two vector layers based on their relationship in space. For example, if we have a `GeoDataFrame` storing field boundaries (polygon geometries) and field attributes and another `GeoDataFrame` storing shire boundaries (polygon geometries) and a shire name as an attribute, we can join the the two tables based on the largest intersection (overlap) between field boundaries and shire boundaries. If the field boundaries `GeoDataFrame` was the left table in the spatial join, for each row (or geometry feature) the shire name from the shire with largest intersection would be joined to that table in a new column. \n",
    "\n",
    "GeoPandas provides an <a href=\"https://geopandas.org/en/stable/docs/user_guide/mergingdata.html#spatial-joins\" target=\"_blank\">`sjoin()` function</a> that can be used for spatial joins of two `GeoDataFrames`. The `sjoin()` function expects the following as arguments:\n",
    "\n",
    "* `left_df` - left `GeoDataFrame` in the spatial join.\n",
    "* `right_df` - right `GeoDataFrame` in the spatial join - columns from the `right_df` will be joined to `left_df`.\n",
    "* `how` - whether to use a left, inner, or right join.\n",
    "* `predicate` - a binary predicate that defines the spatial relationship between features in `right_df` and `left_df`. \n",
    "\n",
    "Binary predicates that can be used are:\n",
    "\n",
    "* intersects\n",
    "* contains\n",
    "* crosses\n",
    "* within\n",
    "* touches\n",
    "* overlaps\n",
    "\n",
    "Intersects is the default predicate for spatial joins in GeoPandas. \n",
    "\n",
    "To complete our data transformation routine we need to add a column to `joined_df` that stores the District that the field is located in. We can do this using a spatial join based on the intersect of the field's centroid (point geometry) and the shape of the District (polygon geometry). \n",
    "\n",
    "But, we need to read in District geometries for India obtained from <a href=\"https://www.geoboundaries.org\" target=\"_blank\">geoBoundaries</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4bc54-6936-4034-b4ec-1096ae267bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "india_districts = gpd.read_file(os.path.join(os.getcwd(), \"week-4\", \"india-adm\", \"geoBoundaries-IND-ADM2_simplified.topojson\"))\n",
    "india_districts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d046e3-d8b7-4610-acd1-35e0d3031e7a",
   "metadata": {},
   "source": [
    "Let's quickly tidy up the India Districts `GeoDataFrame` to keep only the District name and `geometry` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb0854e-7104-48a5-bb3f-91e39e32521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "india_districts = india_districts.loc[:, [\"shapeName\", \"geometry\"]]\n",
    "india_districts.columns = [\"district\", \"geometry\"]\n",
    "india_districts = india_districts.set_crs(\"EPSG:4326\")\n",
    "india_districts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff909b-700d-4eb3-929e-d8d53700c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "india_districts.plot(column=\"district\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce980c-4f2f-4d9d-b826-0ef66a40f768",
   "metadata": {},
   "source": [
    "That looks like India. Let's implement our final data transformation step and perform a spatial join to add a District column to `joined_df`.\n",
    "\n",
    "#### Recap quiz\n",
    "\n",
    "**Use the GeoPandas docs to implement a spatial join with the <a href=\"https://geopandas.org/en/stable/docs/reference/api/geopandas.sjoin.html\" target=\"_blank\">`sjoin()` function</a> that joins `india_districts` (as `right_df`) to `joined_df` (as `left_df`) using an inner join and `intersects` predicate. Assign the result to the variable `joined_df_district`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce58c53-3cff-49db-94c5-7fa585f0fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1459fcb0-20d4-4edb-8c2b-29f041f46256",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```python\n",
    "# spatial join\n",
    "joined_df_district = gpd.sjoin(\n",
    "    left_df=joined_df, \n",
    "    right_df=india_districts, \n",
    "    how=\"inner\", \n",
    "    predicate=\"intersects\"\n",
    ")\n",
    "joined_df_district.head()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6aedf9-341f-489f-b919-09ccbff05499",
   "metadata": {},
   "source": [
    "### Save file\n",
    "\n",
    "Finally, let's write our processed data ready for training and testing a machine learning model to file.\n",
    "\n",
    "#### Recap quiz\n",
    "\n",
    "**Can you save the data referenced by `joined_df_district` to a GeoJSON file on disk? Save the data with the filename `processed_data.geojson` at the path created by `os.path.join(os.getcwd(), \"week-4\", \"processed_data.geojson\")`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12754b53-e854-4cfd-a621-33b9872ed408",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451413cc-e255-443d-bd11-62b1226ad381",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```python\n",
    "# save file\n",
    "out_path = os.path.join(os.getcwd(), \"week-4\", \"processed_data.geojson\")\n",
    "joined_df_district.to_file(out_path)\n",
    "```\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
