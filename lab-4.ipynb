{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f20ccea4-1ef0-4f90-a0c3-3cbe1b0f7adc",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "## Data wrangling and transformation\n",
    "\n",
    "Often, datasets need to go through a series of data wrangling and transformation steps before they are ready for analysis or visualisation tasks. This lab will demonstrate several data wrangling and transformation operations for raster, vector, and tabular data. \n",
    "\n",
    "We will start with a subset of the AgriFieldNet Competition Dataset <a href=\"https://mlhub.earth/data/ref_agrifieldnet_competition_v1\" target=\"_blank\">(Radiant Earth Foundation and IDinsight, 2022)</a> which has been published to encourage people to develop machine learning models that classify a field's crop type from satellite images. This dataset consists of a series of directories with each directory corresponding to a 256 x 256 pixel image footprint. Inside each directory are the following files:\n",
    "\n",
    "* 12 GeoTIFF files corresponding to spectral reflectance in different wavelengths from Sentinel-2 data. \n",
    "* 1 GeoTIFF file with non-zero pixels corresponding to a crop type label. \n",
    "* 1 GeoTIFF file with non-zero pixels corresponding to a field id. \n",
    "* 1 JSON metadata file. \n",
    "\n",
    "This data is subset from a larger dataset covering agricultural fields in four Indian states of Odisha, Uttar Pradesh, Bihar, and Rajasthan. The field boundaries and crop type labels were captured by data collectors from IDinsights Data on Demand team and the satellite image preparation was undertaken by the Radiant Earth Foundation. \n",
    "\n",
    "Our task is to combine all the raster data in separate directories into a tabular dataset that can be used for machine learning tasks to predict a field's crop type. Specifically, we will  transform a disparate collection of GeoTiff files spread across many directories and storing different information into a tabular dataset with columns for each field id, crop type, and average spectral reflectance values in each field for many wavelenghts. We will also store geometry data representing the location of each field in a geometry column. \n",
    "\n",
    "Through completing this task you will use programming skills from week 1, build on your understanding of directories and file systems from week 2, and tools for data cleaning covered in week 3. You will build on these skills by learning a range of common data wrangling and transformation operations which you will be able to apply to a range of datasets to wrangle them into a structure suitable for analysis and visualisation. You will also learn how to use Python to create an automated data transformation routine consisting of several operations to automate the processing of a collection of datasets into a format ready for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dfa67c-251e-47c3-8986-8c7285721ab2",
   "metadata": {},
   "source": [
    "### What is data wrangling?\n",
    "\n",
    "<a href=\"https://r4ds.had.co.nz/wrangle-intro.html\" target=\"_blank\">Wickham and Grolemund (2017)</a> and <a href=\"https://wesmckinney.com/book/\" target=\"_blank\">McKinney (2022)</a> state that data wrangling consists of data import, data cleaning, and data transformation. \n",
    "\n",
    "#### Data import\n",
    "\n",
    "Data import was covered in week 2 with examples of how to read tabular, vector, and raster data into Python programs. \n",
    "\n",
    "#### Data cleaning\n",
    "\n",
    "Data cleaning was covered in week 3 as part of the exploratory data analysis with examples of how to handle outliers and missing data. \n",
    "\n",
    "#### Data transformation\n",
    "\n",
    "<a href=\"https://wesmckinney.com/book/\" target=\"_blank\">McKinney (2022)</a> define data transformation as the application of mathematical or statistical operations to data to generate new datasets. Data transformation can also include operations that reshape datasets or combine two or more datasets.\n",
    "\n",
    "As we're working with spatial and non-spatial data we can categorise data transformation operations as attribute operations, spatial operations, geometry operations, and raster-vector  operations (<a href=\"https://geocompr.robinlovelace.net/index.html\" target=\"_blank\">Lovelace et al. (2022)</a>).\n",
    "\n",
    "**Attribute operations** are applied to non-spatial (attribute data). This could be a tabular dataset without any spatial information, the attribute table of a vector dataset, or the pixel values of a raster dataset. Common attribute operations include:\n",
    "\n",
    "* Selecting columns from a table based on a condition. \n",
    "* Selecting (subsetting) pixels from a raster based on a condition.\n",
    "* Filtering rows from a table based on a condition. \n",
    "* Creating a new column of values using a function applied to existing data.\n",
    "* Computing summary statistics of columns in a table or of pixel values in a raster.\n",
    "* Joining datasets based on matching values in columns (keys).\n",
    "\n",
    "**Spatial operations** transform data using the data's geographic information including shape and location. Vector spatial operations include:\n",
    "\n",
    "* Spatial subsetting by selecting data points based on a geographic condition (e.g. selecting all fields in Western Australia).\n",
    "* Spatial joins where datasets are combined based on their relationship in space. \n",
    "* Spatial aggregation where summaries are produced for regions (e.g. the average crop yield for all fields in a region).\n",
    "\n",
    "Spatial operations on raster data are based on map algebra concepts and include:\n",
    "\n",
    "* Local operations which are applied on a pixel by pixel basis (e.g. converting a raster of temperature values in °F to °C).\n",
    "* Focal operations which summarise or transform a raster value using the values of neihbouring pixels (e.g. computing the average value within a 3 x 3 pixel moving window).\n",
    "* Zonal operations which summarise or transform raster values using values inside an irregular shaped zone.\n",
    "* Global operations which summarise the entire raster (e.g. computing the minimum value in the raster dataset). \n",
    "\n",
    "**Geometry operations** transform a dataset's geographic information. Common geometry operations for vector data include:\n",
    "\n",
    "* Simplification of shapes.\n",
    "* Computing the centroid of polygons.\n",
    "* Clipping (subsetting) of geometries based on their intersection or relationship with another geometry. \n",
    "\n",
    "and geometry operations on raster data typically involve changing the spatial resolution and include:\n",
    "\n",
    "* Aggregation or dissagregation.\n",
    "* Resampling\n",
    "\n",
    "**Raster-vector operations** involve both raster and vector datasets and include:\n",
    "\n",
    "* Cropping or masking raster data using a vector geometry.\n",
    "* Extracting raster values that intersect with a vector geometry.\n",
    "* Rasterisation where a vector dataset is transformed to a raster layer.\n",
    "* Vectorisation where a raster dataset is transformed to a vector layer.\n",
    "\n",
    "### Feature engineering\n",
    "\n",
    "Feature engineering is part of a machine learning workflow which involves preparing and preprocessing datasets ready for machine learning model training and evaluation. Feature engineering is one example where data wrangling operations are applied. This lab can be considered a feature engineering task where a range of raster satellite image datasets are transformed to a vector-tabular dataset which can be used to train and test a machine learning model. Often, datasets for machine learning computer vision tasks (e.g. see the datasets on Radiant Earth's <a href=\"https://mlhub.earth/\" target=\"_blank\">MLHub</a>) are provided with data samples for model development spread across many sub-directories. Prior to model training you need to extract the data from these directories and assemble it in a way that it can be passed into a model. You can use this lab as a starter template for these kind of feature engineering tasks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f2b402-819a-47b4-a1d2-0d0e2209a220",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Run the labs\n",
    "\n",
    "You can run the labs locally on your machine or you can use cloud environments provided by Google Colab or Binderhub. **If you're working with Google Colab and Binderhub be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.**\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/data-analysis-3300-3003/colab/blob/main/lab-4.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "<a href=\"https://mybinder.org/v2/gh/binder-3300-3003/binder/HEAD\" target=\"_blank\">\n",
    "  <img src=\"https://mybinder.org/badge_logo.svg\" alt=\"Open In Binder\"/>\n",
    "</a>\n",
    "\n",
    "### Download data\n",
    "\n",
    "If you need to download the data for this lab, run the following code snippet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6417f87-ebf2-4955-b593-5e2ccc7748ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"week-4\" not in os.listdir(os.getcwd()):\n",
    "    os.system('wget \"https://github.com/data-analysis-3300-3003/data/raw/main/data/week-4.zip\"')\n",
    "    os.system('unzip \"week-4.zip\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c646b92f-1966-4904-b221-f4715630f341",
   "metadata": {},
   "source": [
    "### Working in Colab\n",
    "\n",
    "If you're working in Google Colab, you'll need to run the following code snippet to install the required packages that don't come with the colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e478e4-615a-4713-bae1-75cd440e48b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopandas\n",
    "!pip install pyarrow\n",
    "!pip install mapclassify\n",
    "!pip install rasterio\n",
    "!pip install libpysal\n",
    "!pip install esda\n",
    "!pip install splot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf742d0-c865-4407-9551-b35cd21a6774",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e0488-dedf-4e03-9cb3-363cffec6b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import plotly.io as pio\n",
    "import shapely.geometry\n",
    "import pprint\n",
    "\n",
    "from rasterio import features\n",
    "\n",
    "pio.renderers.default = \"colab\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283743b5-18c6-4260-8569-a8e838bc932b",
   "metadata": {},
   "source": [
    "## Data storage\n",
    "\n",
    "In week 2 we showed that files are organised within a hierarchy of directories and sub-directories (or folders) in a computer system. Each sample (256 x 256 pixel image) is stored in its own sub-directory. We can explore the structure of these directories and how files are arranged within it. \n",
    "\n",
    "First, let's list the first level of sub-directories within the `week-4` folder. Each of these sub-directories corresponds to a 256 x 256 pixel image.\n",
    "\n",
    "To recap, we can use functions provided by the `os` package to help us explore directories. The `os.path.join()` function takes a sequence of string data representing directory names or file names and combines them into a path. The `os.listdir()` function lists all files or sub-directories in a directory pointed to by a path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d62be97-a8fe-41f5-a600-5d4108c45164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image and labels data\n",
    "data_path = os.path.join(os.getcwd(), \"week-4\", \"images\")\n",
    "\n",
    "image_dirs = os.listdir(data_path)\n",
    "for i in image_dirs:\n",
    "    if i != \".DS_Store\":\n",
    "        print(i)\n",
    "    \n",
    "if \".DS_Store\" in image_dirs:\n",
    "    image_dirs.remove(\".DS_Store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5be2da-de7e-4f00-b733-bcd66fd7b947",
   "metadata": {},
   "source": [
    "The `os.listdir()` function lists files or directories at the first level of the directory passed into the function as an argument. However, often we have nested directory structures consisting of a hierarchy of sub-directories and files. The `os.walk()` can be used to traverse a directory tree. We can use the `os.walk()` function to fully reveal how the files are arranged within a sub-directory. \n",
    "\n",
    "We can see that within each sub-directory corresponding to a 256 x 256 pixel image footprint there are the following files and sub-directories:\n",
    "\n",
    "* Files with the names `B*.tif` which are Sentinel-2 images for a particular waveband. For example, B02.tif is Sentinel-2 reflectance in the blue visible wavelength. These files will be used to generate predictor variables in a subsequent machine learning task to predict crop type.\n",
    "* A `field` sub-directory which contains a `field_ids.tif` file. This file stores field ids as pixel values. \n",
    "* A `label` sub-directory which contains a `raster_labels.tif` file. This file stores a numeric indicator of crop type as pixel values. These files store target labels used to train and test a machine learning model that predicts crop type from spectral reflectance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c9737-4b9f-4945-96fc-5c16b554bfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(os.path.join(data_path, image_dirs[1]), topdown=True):\n",
    "    print(root)\n",
    "    for f in files:\n",
    "        print(f\"    {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2814e3b5-00a1-4d2a-a712-b09a2661ba87",
   "metadata": {},
   "source": [
    "### Data visualisation\n",
    "\n",
    "Let's quickly visualise some of this data to get an idea of its structure. First, let's visualise the Sentinel-2 satellite image data starting with the image storing reflectance in the green visible portion of the electromagnetic spectrum. \n",
    "\n",
    "We can see that the image shape is 256 x 256 pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832252ac-4328-4ae8-97ba-6a6ea9c6ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the green band GeoTIFF file\n",
    "s2_green_path = os.path.join(os.getcwd(), \"week-4\", \"images\", image_dirs[1], \"B03.tif\")\n",
    "\n",
    "# open the green band GeoTIFF file and read its image data\n",
    "with rasterio.open(s2_green_path) as src:\n",
    "    green_band = src.read(1)\n",
    "    print(f\"the shape of the image is {green_band.shape}\")\n",
    "\n",
    "# plot the green band\n",
    "px.imshow(green_band, color_continuous_scale=\"Greens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d8b127-fa99-446a-b700-e988d7cfcbfe",
   "metadata": {},
   "source": [
    "Let's also create a true colour composite image using the red, green, and blue band images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a566df4e-be56-4c1f-bcc0-a02d78800c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the red band GeoTIFF file\n",
    "s2_red_path = os.path.join(os.getcwd(), \"week-4\", \"images\", image_dirs[1], \"B04.tif\")\n",
    "\n",
    "# open the red band GeoTIFF file and read its image data\n",
    "with rasterio.open(s2_red_path) as src:\n",
    "    red_band = src.read()\n",
    "\n",
    "# path to the green band GeoTIFF file\n",
    "s2_green_path = os.path.join(os.getcwd(), \"week-4\", \"images\", image_dirs[1], \"B03.tif\")\n",
    "\n",
    "# open the green band GeoTIFF file and read its image data\n",
    "with rasterio.open(s2_green_path) as src:\n",
    "    green_band = src.read()\n",
    "    \n",
    "# path to the blue band GeoTIFF file\n",
    "s2_blue_path = os.path.join(os.getcwd(), \"week-4\", \"images\", image_dirs[1], \"B02.tif\")\n",
    "\n",
    "# open the blue band GeoTIFF file and read its image data\n",
    "with rasterio.open(s2_blue_path) as src:\n",
    "    blue_band = src.read()\n",
    "    \n",
    "# make RGB image\n",
    "rgb = np.concatenate((red_band, green_band, blue_band), axis=0)\n",
    "\n",
    "# plot the rgb image\n",
    "px.imshow(np.moveaxis(rgb, 0, 2), contrast_rescaling=\"minmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1593d746-a3f9-4bab-a4fb-1e4ebade72b2",
   "metadata": {},
   "source": [
    "Now, let's explore the field id images. We can see that the image contains a few fields with ids assigned to them. Hover over each field and you can see its numeric id value. However, we can also see that a large portion of the image is covered by pixels with the value 0. These locations are not labelled fields and we will need to drop them from our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2272cb08-549a-4532-b5a7-abc5e7b9dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the GeoTIFF file\n",
    "field_id_path = os.path.join(os.getcwd(), \"week-4\", \"images\", image_dirs[1], \"field/field_ids.tif\")\n",
    "\n",
    "# open the GeoTIFF file and read its image data\n",
    "with rasterio.open(field_id_path) as src:\n",
    "    field_id_band = src.read(1)\n",
    "\n",
    "# plot the field id band\n",
    "px.imshow(field_id_band)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922511ff-ba68-4b45-a75e-1ddf4012f321",
   "metadata": {},
   "source": [
    "Finally, we can look at our labels image. Each field's pixels are assigned a numeric value that corresponds to a crop type. Based on the dataset's documentation the below is the mapping between numeric values and crop types in the labels dataset. \n",
    "\n",
    "* 1 - Wheat\n",
    "* 2 - Mustard\n",
    "* 3 - Lentil\n",
    "* 4 - No crop/Fallow\n",
    "* 5 - Green pea\n",
    "* 6 - Sugarcane\n",
    "* 8 - Garlic\n",
    "* 9 - Maize\n",
    "* 13 - Gram\n",
    "* 14 - Coriander\n",
    "* 15 - Potato\n",
    "* 16 - Bersem\n",
    "* 36 - Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66c7454-26b3-4dec-97b0-7a3d5a27d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the GeoTIFF file\n",
    "label_path = os.path.join(os.getcwd(), \"week-4\", \"images\", image_dirs[1], \"label/raster_labels.tif\")\n",
    "\n",
    "# open the GeoTIFF file and read its metadata and image data\n",
    "with rasterio.open(label_path) as src:\n",
    "    meta = src.meta\n",
    "    label_band = src.read(1)\n",
    "\n",
    "# Plot the crop label band\n",
    "px.imshow(label_band)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a3c0f-24a2-4384-a55d-14a9c78c4f6a",
   "metadata": {},
   "source": [
    "## Raster data processing\n",
    "\n",
    "### NumPy refresher\n",
    "\n",
    "In week 2 we introduced NumPy `ndarray` objects for storing multidimensional (or N-dimensional) arrays which consist of a grid of elements of the same data type. `ndarray` objects are a logical data structure for representing and manipulating raster and image data in Python programs.\n",
    "\n",
    "The dimensions of a NumPy `ndarray` are called axes. A single band raster layer would have two axes, rows would be arranged along the 0th axis and columns along the 1st axis. The shape of an `ndarray` refers to the number of elements along each axis. \n",
    "\n",
    "Let's quickly revise these concepts by working with a raster layer with 3 rows and 3 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211505b1-6ac5-4846-a742-facadbb3b235",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_raster = np.array(\n",
    "    [[1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]])\n",
    "print(demo_raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19277458-33c6-4319-bf46-28a3045127f3",
   "metadata": {},
   "source": [
    "This `ndarray` object should have 2 axes corresponding to rows (0 axis) and columns (1 axis) with 3 elements along each axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c36ea21-4ca3-491b-97ed-c9a50c6a2e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"the shape of demo_raster is {demo_raster.shape}\")\n",
    "print(f\"the number of elements on the 0 axis (rows) is {demo_raster.shape[0]}\")\n",
    "print(f\"the number of elements on the 1 axis (columns) is {demo_raster.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e200f323-7125-4f5b-b999-9ea7d5b38337",
   "metadata": {},
   "source": [
    "### Subsetting NumPy `ndarray`s\n",
    "\n",
    "This is a form of subsetting opertation where you select values from a NumPy `ndarray` object based on their index locations. These operations are generally referred to as indexing and slicing when working with NumPy `ndarray` objects. <a href=\"https://geocompr.robinlovelace.net/index.html\" target=\"_blank\">Lovelace et al. (2022)</a> refer to these operations on raster data as *row-column subsetting*.\n",
    "\n",
    "We can extract a value from a NumPy `ndarray` based on its index location. For example, the first element of a 2-Dimensional `ndarray` is at location `[0, 0]` (i.e. the 0th row and 0th column). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b295b3-f05f-4020-b32a-95ff8b922c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_element = demo_raster[0, 0]\n",
    "print(first_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3216fb93-870a-467c-989b-2c40b844949d",
   "metadata": {},
   "source": [
    "We can use the `:` symbol to specify slices of a NumPy `ndarray` to subset. For example, the following are three different ways of slicing the first two rows.\n",
    "\n",
    "Note that the slice is not inclusive of the index location after the `:` symbol. So, `demo_raster[0:2, ]` would select the first two rows of `demo_raster` - row 0 and row 1 (remember Python indexes from 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da4de28-d13f-4041-bc33-42194bf2c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_rows_1 = demo_raster[0:2, ]\n",
    "print(two_rows_1)\n",
    "\n",
    "two_rows_2 = demo_raster[0:2]\n",
    "print(two_rows_2)\n",
    "\n",
    "two_rows_3 = demo_raster[:2]\n",
    "print(two_rows_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce22181-f234-44d4-ae8d-163a40eb3099",
   "metadata": {},
   "source": [
    "We can use multiple slices for different axes. For example, if we wanted to subset values from a selection of rows and columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fd2c7e-6b80-4552-8a7d-f5c7ea6ee3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_rows_cols = demo_raster[:2, 1:]\n",
    "print(two_rows_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27edc694-f104-4b4b-be91-5099951ac543",
   "metadata": {},
   "source": [
    "### Band stacking \n",
    "\n",
    "A common data wrangling and combination operation when working with raster data is band stacking. This is the process of taking two or more single band rasters and stacking them to create a multiband raster. \n",
    "\n",
    "When using NumPy `ndarray` objects to handle raster data, this process could involve stacking two `ndarray` objects with a shape of `(256, 256)` to create a new `ndarray` object with the shape `(2, 256, 256)`. Here, axis 0 has a length of two which indicates that we've stacked two `ndarray` objects with the shape `(256, 256)`.\n",
    "\n",
    "We can use the NumPy `stack()` and `concatenate()` functions to combine a sequence of `ndarray`s along an axis. \n",
    "\n",
    "We can write a short code snippet to start our automated data transformation routine that will loop over image directories and Sentinel-2 bands, read the data stored in GeoTIFF files corresponding to Sentinel-2 reflectance values into an `ndarray`, and then use NumPy's `stack()` function to stack a list of `ndarray`s representing a multiband raster structure. Let's break this down step by step:\n",
    "\n",
    "We loop over our list of directories (named in the list `image_dirs`) and inside each directory are a series of `B*.tif` files storing Sentinel-2 reflectance data. The outer loop over directories is: \n",
    "\n",
    "```python\n",
    "for i in image_dirs:\n",
    "    ## When inside this loop i takes on a directory name listed in image_dirs\n",
    "    print(f\"stacking Sentinel-2 bands in {i}\")\n",
    "```\n",
    "\n",
    "Next, we create an empty list which we'll use to store `ndarray` objects of data read from the `B*.tif` GeoTIFF files. This is defined as `bands = []`, the `[]` creates an empty list:\n",
    "\n",
    "```python\n",
    "for i in image_dirs:\n",
    "    print(f\"stacking Sentinel-2 bands in {i}\")\n",
    "    # a list to store numpy ndarray objects of Sentinel-2 reflectance data for each band\n",
    "    bands = []\n",
    "```\n",
    "\n",
    "Now, for each iteration of `i`, which corresponds to a directory, we iterate over a the `B*.tif` files inside `i` and read the data stored in each file into an `ndarray` and append the `ndarray` to the `bands` list using the `bands.append(src.read(1))` command. \n",
    "\n",
    "```python\n",
    "for i in image_dirs:\n",
    "    print(f\"stacking Sentinel-2 bands in {i}\")\n",
    "    # a list to store numpy ndarray objects of Sentinel-2 reflectance data for each band\n",
    "    bands = []\n",
    "    \n",
    "    # loop over each band, read in the data from the corresponding GeoTIFF file into an ndarray\n",
    "    for b in s2_bands:\n",
    "        band_path = os.path.join(os.getcwd(), data_path, i, b + \".tif\")\n",
    "        with rasterio.open(band_path) as src:\n",
    "            # append the ndarray storing the Sentinel-2 reflectance data for a band to a list\n",
    "            bands.append(src.read(1))\n",
    "```\n",
    "\n",
    "You will notice the use of the context manager denoted by the `with` block to read data from GeoTIFF files into a NumPy `ndarray`. This was covered in week 2, but let's revise this quickly: \n",
    "\n",
    "```python\n",
    "with rasterio.open(band_path) as src:\n",
    "```\n",
    "\n",
    "Creates a file object referenced by `src` which points to the file at the path referenced by `band_path` (here a GeoTIFF file). The use of the `with` block as a context manager takes care of closing the connection `src` when we have finished reading data from the file. The file object `src` has a `read()` method which can be used to read data from the file it connects to into our Python program. \n",
    "\n",
    "```python\n",
    "src.read(1)\n",
    "```\n",
    "\n",
    "For each `B*.tif` file in a directory, we read it's data into our program as: \n",
    "\n",
    "```python\n",
    "for b in b2_bands:\n",
    "    band_path = os.path.join(os.getcwd(), data_path, i, b + \".tif\")\n",
    "    with rasterio.open(band_path) as src:\n",
    "        # append the ndarray storing the Sentinel-2 reflectance data for a band to a list\n",
    "        bands.append(src.read(1))\n",
    "```\n",
    "\n",
    "Once we have finished looping over the bands in the directory `i`, we can stack the `ndarray` objects in the list `bands` to form a NumPy `ndarray` representation of a multiband raster. We do this by passing the list of `ndarray` objects into the NumPy `stack()` function.\n",
    "\n",
    "```python\n",
    "# loop over image directories\n",
    "for i in image_dirs:\n",
    "    print(f\"stacking Sentinel-2 bands in {i}\")\n",
    "    # a list to store numpy ndarray objects of Sentinel-2 reflectance data for each band\n",
    "    bands = []\n",
    "    \n",
    "    # loop over each band, read in the data from the corresponding GeoTIFF file into an ndarray\n",
    "    for b in s2_bands:\n",
    "        band_path = os.path.join(os.getcwd(), data_path, i, b + \".tif\")\n",
    "        with rasterio.open(band_path) as src:\n",
    "            # append the ndarray storing the Sentinel-2 reflectance data for a band to a list\n",
    "            bands.append(src.read(1))\n",
    "    \n",
    "    # stack all bands in the list to create a multiband raster\n",
    "    bands = np.stack(bands)\n",
    "```\n",
    "\n",
    "Finally, before we more onto the next `i` (i.e. the next directory in the list `image_dirs`) and repeat this process, we append our `ndarray` object of stacked Sentinel-2 bands to the list `stacked_bands`. When we have finished looping over all directories in `image_dirs` the list `stacked_bands` will be a collection of `ndarray` objects that we can refer to later in our program.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8077d761-5404-4ff1-a8e7-6fa2c81a6c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking bands \n",
    "\n",
    "# Sentinel-2 band names \n",
    "s2_bands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "\n",
    "# a list to store numpy ndarray objects representing multiband rasters of Sentinel-2 reflectance data\n",
    "stacked_bands = []\n",
    "\n",
    "# loop over image directories\n",
    "for i in image_dirs:\n",
    "    print(f\"stacking Sentinel-2 bands in {i}\")\n",
    "    # a list to store numpy ndarray objects of Sentinel-2 reflectance data for each band\n",
    "    bands = []\n",
    "    \n",
    "    # loop over each band, read in the data from the corresponding GeoTIFF file into an ndarray\n",
    "    for b in s2_bands:\n",
    "        band_path = os.path.join(os.getcwd(), data_path, i, b + \".tif\")\n",
    "        with rasterio.open(band_path) as src:\n",
    "            # append the ndarray storing the Sentinel-2 reflectance data for a band to a list\n",
    "            bands.append(src.read(1))\n",
    "    \n",
    "    # stack all bands in the list to create a multiband raster\n",
    "    bands = np.stack(bands)\n",
    "    \n",
    "    # append the multiband raster to the list storing multiband rasters for data in each directory\n",
    "    stacked_bands.append(bands)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6f9a94-606d-4320-bdb8-a4c241e58368",
   "metadata": {},
   "source": [
    "Let's inspect the output of the band stacking workflow. The list `stacked_bands` should be a list of `ndarray` objects with rank 3. Each `ndarray` object should have three axes (bands, rows, columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb7ad24-3719-4e0b-85d3-cb3e8e984a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "for i in stacked_bands:\n",
    "    print(f\"the shape of the ndarray at position {idx} in stacked_bands is {i.shape} with rank {len(i.shape)}\")\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79be0c-2e52-458b-a66b-1b68df2b7a76",
   "metadata": {},
   "source": [
    "### Map algebra\n",
    "\n",
    "Following <a href=\"https://geocompr.robinlovelace.net/index.html\" target=\"_blank\">Lovelace et al. (2022)</a>, we refer to map algebra as operations that transform raster pixel values via statistical or mathematical operations which can involve combining pixel values from different raster layers or using neighbouring raster values. \n",
    "\n",
    "Local map algebra operations operate on a pixel by pixel basis; the mathematical operation is applied independently to each pixel withour reference to neighbouring pixel values. For example, addition, subtraction, multiplication, and logical operations can all be applied on a pixel by pixel basis. \n",
    "\n",
    "A commonly used local operation when working with remote sensing data is computing spectral indices. Spectral indices are pixel by pixel mathematical combinations of spectral reflectance in different wavelenghts that are used to emphasise and reveal vegetation or land surface conditions. The normalised difference vegetation index (NDVI) is used for tracking vegetation condition and representing the greenness of vegetation in a remote sensing image. \n",
    "\n",
    "As we're processing these remote sensing images to generate characteristics of fields that can be used to predict crop type, we will also compute each field's NDVI value as it could contain useful information to discriminate between crop types. **You will learn more about the theory and use of spectral and vegetation indices for remote sensing image analysis in GEOG 3301**. \n",
    "\n",
    "The NDVI is computed as:\n",
    "\n",
    "$NDVI = \\frac{NIR-red}{NIR+red}$\n",
    "\n",
    "Thus, the NDVI is computed via division, subtraction, and addition operations computed on a pixel by pixel basis using raster data corresponding to red and near infrared reflectance. \n",
    "\n",
    "NumPy provides tools for fast mathematical operations on `ndarray` objects. If `ndarray` objects have the same shape, a mathematical combination of two or more `ndarray` objects will be computed on an element by element (i.e. pixel by pixel) basis without needing to write for loops to iterate over `ndarray` elements. This feature of NumPy is called *vectorisation* and makes NumPy a useful tool for processing and analysing large amounts of image data.\n",
    "\n",
    "For example, if we wanted to add two NumPy `ndarray` objects together on a pixelwise basis we could do this using for loops in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dffe4bd-203f-4201-8ff1-236dac2ff4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow array addition using for loops\n",
    "a = np.array(\n",
    "    [[1, 2],\n",
    "     [3, 4]])\n",
    "\n",
    "b = np.array(\n",
    "    [[1, 2],\n",
    "     [3, 4]])   \n",
    "\n",
    "result = np.zeros((2, 2))\n",
    "\n",
    "for r in range(0, 2):\n",
    "    print(f\"processing row {r}\")\n",
    "    for c in range(0, 2):\n",
    "        print(f\"processing column {c}\")\n",
    "        \n",
    "        result[r, c] = a[r, c] + b[r, c]\n",
    "\n",
    "print(result)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b4c1e2-e1ce-494d-8f07-218597e43bed",
   "metadata": {},
   "source": [
    "However, this approach will be slow if we are working with large raster datasets in NumPy `ndarray`s. It is also quite verbose, we need to write two for loops just to perform element-wise addition of two arrays. These element-wise operations can be computed in isolation. In other words adding the elements in pixel location `0, 0` does not depend on the result of adding elements in pixel location `0, 1`. This means that element-wise operations can be performed in parallel, which is termed vectorised computation in NumPy, and you can use NumPy's element-wise operations to perform mathematical operations on `ndarray`s in parallel. \n",
    "\n",
    "This has two advantages: speed (particularly when working with large or many images) and cleaner code. The NumPy element-wise approach to adding the `ndarray`s `a` and `b` above is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448de5c2-f6ba-4a38-b6ff-66f616a70b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorised addition using for loops\n",
    "a = np.array(\n",
    "    [[1, 2],\n",
    "     [3, 4]])\n",
    "\n",
    "b = np.array(\n",
    "    [[1, 2],\n",
    "     [3, 4]])  \n",
    "\n",
    "result = a + b\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e65107-2b26-4ace-80d5-42e2fb3b8926",
   "metadata": {},
   "source": [
    "Now we're ready to use NumPy's element wise operations to compute the NDVI. First, let's do this for a single image before extending the routine to automate the generation of NDVI bands for all images. We'll get the first `ndarray` object from `stacked_bands`, a list of rank 3 (bands, rows, cols) `ndarray` objects with each band storing reflectance in different wavelengths. \n",
    "\n",
    "Red reflectance is stored in band 4 of Sentinel-2 images. Thus, red reflectance would be located at index position 3 (the fourth element as we start indexing at 0) on axis 0. \n",
    "\n",
    "Near infrared reflectance is stored in band 8 of Sentinel-2 images. By the same logic near infrared reflectance is located at index position 7 on axis 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac40cf4-18a1-43bc-acbd-0bd59434a4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute NDVI\n",
    "image_1 = stacked_bands[0]\n",
    "print(f\"the shape of the first ndarray (image_1) in stacked bands is {image_1.shape}\")\n",
    "\n",
    "# get the red band\n",
    "red = image_1[3, :, :].astype(\"float64\")\n",
    "\n",
    "# get the nir band\n",
    "nir = image_1[7, :, :].astype(\"float64\")\n",
    "\n",
    "# compute the ndvi\n",
    "ndvi = (nir-red)/(nir+red)\n",
    "\n",
    "print(f\"the shape of the ndvi image is {ndvi.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f194fca7-aee3-43a3-9738-9ff18e2792f2",
   "metadata": {},
   "source": [
    "NDVI values fall between -1 and 1 with values closer to 1 indicating greener vegetation and values less than 0 indicating an absence of vegetation. Let's visualise the NDVI band and check the values fall within this range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d265893a-7e3d-4d77-9c81-800b8bdbdd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the ndvi image\n",
    "px.imshow(ndvi, color_continuous_scale=\"viridis\", contrast_rescaling=\"minmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0acc22a-3674-40b3-a2c7-885601926b28",
   "metadata": {},
   "source": [
    "You will have noticed that prior to computing the NDVI values we converted the red and near infrared `ndarray` objects to `float64` type. This is because NDVI values represent variation in vegetation conditions using numbers with digits before and after the decimal point (a NDVI value of 0.8 would indicate green vegetation and a NDVI value of -0.2 would indicate an absence of green vegetation and possibly water cover). Thus, it is more appropriate to use floating point numbers to store NDVI values.\n",
    "\n",
    "If you check the data type (`dtype`) of the values of the multiband `ndarray` objects storing reflectance data you will see that they are of type `uint8`. This means they are unsigned integers - they cannot represent negative values. However, NDVI values can be negative. Therefore, we should convert these values to a data type that can store negative and positive (i.e. signed) numbers. \n",
    "\n",
    "We use the NumPy method `astype()` to cast an `ndarray` to a new `dtype`. You can read more about NumPy data types <a href=\"https://wesmckinney.com/book/numpy-basics.html#numpy_dtypes\" target=\"_blank\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ec2e7-1f2c-44e0-a194-4fd99db5544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The dtype of the red band is {image_1[3, :, :].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed3bf9c-1aa9-4461-add7-1c5a03e0712c",
   "metadata": {},
   "source": [
    "Now we have demonstrated how to compute NDVI values from NumPy `ndarray` objects, we can edit our existing routine that created rank 3 `ndarray` objects stacking by raster layers to also include an NDVI band. This will create a routine that not only automates the stacking of multiple GeoTIFF files in different directories but also the computation of NDVI bands for each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10695be9-146c-4816-9cd8-29b1c3e3accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking bands \n",
    "\n",
    "# Sentinel-2 band names \n",
    "s2_bands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "\n",
    "# a list to store numpy ndarray objects representing multiband rasters of Sentinel-2 reflectance data\n",
    "stacked_bands = []\n",
    "\n",
    "# loop over image directories\n",
    "for i in image_dirs:\n",
    "    print(f\"stacking Sentinel-2 bands in {i}\")\n",
    "    # a list to store numpy ndarray objects of Sentinel-2 reflectance data for each band\n",
    "    bands = []\n",
    "    \n",
    "    # loop over each band, read in the data from the corresponding GeoTIFF file into an ndarray\n",
    "    for b in s2_bands:\n",
    "        band_path = os.path.join(os.getcwd(), data_path, i, b + \".tif\")\n",
    "        with rasterio.open(band_path) as src:\n",
    "            # append the ndarray storing the Sentinel-2 reflectance data for a band to a list\n",
    "            bands.append(src.read(1))\n",
    "    \n",
    "    # stack all bands in the list to create a multiband raster\n",
    "    bands = np.stack(bands)\n",
    "    \n",
    "    # make NDVI band\n",
    "    red = bands[3,:,:].astype(\"float64\")\n",
    "    nir = bands[7,:,:].astype(\"float64\")\n",
    "    ndvi = (nir-red)/(nir+red)\n",
    "    ndvi = np.expand_dims(ndvi, axis=0) # add a bands axis\n",
    "    bands = np.concatenate((bands, ndvi), axis=0) # stack the ndvi band\n",
    "    \n",
    "    # append the multiband raster to the list storing multiband rasters for data in each directory\n",
    "    stacked_bands.append(bands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec668557-9925-4356-ba14-d8fdf3e949e8",
   "metadata": {},
   "source": [
    "Let's visualise the NDVI band in the first `ndarray` in `stacked_bands` to check it looks sensible. Also, note how we can use the index value `-1` for the last element along an axis. The NDVI band was stacked at the end of the 0 axis of the `ndarray` so the NDVI raster is the last slice of the `ndarray` on the 0 axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024564c-20e8-4c91-b0b9-56c1a26dec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_raster = stacked_bands[0]\n",
    "print(f\"the shape of the first ndarray in stacked_bands is {check_raster.shape}\")\n",
    "\n",
    "ndvi = check_raster[-1, :,  :]\n",
    "\n",
    "# visualise the ndvi image\n",
    "px.imshow(ndvi, color_continuous_scale=\"viridis\", contrast_rescaling=\"minmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f10285-c7b1-4bb3-9176-4670fa1a37b5",
   "metadata": {},
   "source": [
    "### Labels and field id bands\n",
    "\n",
    "So, far we have built a routine that automates the processing of Sentinel-2 images using a series of operations applied to raster data. These are our predictor variables of crop type in a dataset that can be used to train a machine learning model that classifies crop type. Now we need to get the crop type labels that our model will learn to predict (classify) from the Sentinel-2 remote sensing data. \n",
    "\n",
    "We can extend our existing routine that iterates over directories listed in `image_dirs` to do this. After we generate and stack the NDVI band we can read in the field id and crop type labels data and append them to the `ndarray` object too. \n",
    "\n",
    "Look for the following code in the routine below to see how this is done: \n",
    "\n",
    "```python\n",
    "### HERE WE ARE STACKING THE FIELD ID BAND ###\n",
    "field_id_path = os.path.join(os.getcwd(), data_path, i, \"field/field_ids.tif\")\n",
    "with rasterio.open(field_id_path) as src:\n",
    "    field_ids = src.read().astype(\"float64\")\n",
    "    field_ids[field_ids == 0] = np.nan\n",
    "    bands = np.concatenate((bands, field_ids), axis=0)\n",
    "\n",
    "### HERE WE ARE STACKING THE CROP TYPE LABELS BAND ###\n",
    "labels_path = os.path.join(os.getcwd(), data_path, i, \"label/raster_labels.tif\")\n",
    "with rasterio.open(labels_path) as src:\n",
    "    bands = np.concatenate((bands, src.read()), axis=0)\n",
    "```\n",
    "\n",
    "A few things to note:\n",
    "\n",
    "* the GeoTIFF files storing the field ids (`field_ids.tif`) and crop type labels (`raster_labels.tif`) are stored in sub-directories within each directory referenced by `i`. This means we need to create a path that points to these files within their sub-directories.\n",
    "* pixels in the `field_ids.tif` raster with a value of 0 do not correspond to crop type labels. These pixels are not of interest to us here so we can set them to `np.nan`. Later we will drop all `np.nan` values so our dataset only reflects locations with crop type labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd2e99-e950-4fb3-a8d6-c689377d646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking bands \n",
    "\n",
    "# Sentinel-2 band names \n",
    "s2_bands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "\n",
    "# a list to store numpy ndarray objects representing multiband rasters of Sentinel-2 reflectance data\n",
    "stacked_bands = []\n",
    "\n",
    "# loop over image directories\n",
    "for i in image_dirs:\n",
    "    print(f\"stacking Sentinel-2 bands in {i}\")\n",
    "    # a list to store numpy ndarray objects of Sentinel-2 reflectance data for each band\n",
    "    bands = []\n",
    "    \n",
    "    # loop over each band, read in the data from the corresponding GeoTIFF file into an ndarray\n",
    "    for b in s2_bands:\n",
    "        band_path = os.path.join(os.getcwd(), data_path, i, b + \".tif\")\n",
    "        with rasterio.open(band_path) as src:\n",
    "            # append the ndarray storing the Sentinel-2 reflectance data for a band to a list\n",
    "            bands.append(src.read(1))\n",
    "    \n",
    "    # stack all bands in the list to create a multiband raster\n",
    "    bands = np.stack(bands)\n",
    "    \n",
    "    # make NDVI band\n",
    "    red = bands[3,:,:].astype(\"float64\")\n",
    "    nir = bands[7,:,:].astype(\"float64\")\n",
    "    ndvi = (nir-red)/(nir+red)\n",
    "    ndvi = np.expand_dims(ndvi, axis=0) # add a bands axis\n",
    "    bands = np.concatenate((bands, ndvi), axis=0) # stack the ndvi band\n",
    "    \n",
    "    ### HERE WE ARE STACKING THE FIELD ID BAND ###\n",
    "    field_id_path = os.path.join(os.getcwd(), data_path, i, \"field/field_ids.tif\")\n",
    "    with rasterio.open(field_id_path) as src:\n",
    "        field_ids = src.read().astype(\"float64\")\n",
    "        field_ids[field_ids == 0] = np.nan\n",
    "        bands = np.concatenate((bands, field_ids), axis=0)\n",
    "    \n",
    "    ### HERE WE ARE STACKING THE CROP TYPE LABELS BAND ###\n",
    "    labels_path = os.path.join(os.getcwd(), data_path, i, \"label/raster_labels.tif\")\n",
    "    with rasterio.open(labels_path) as src:\n",
    "        bands = np.concatenate((bands, src.read()), axis=0)\n",
    "    \n",
    "    # append the multiband raster to the list storing multiband rasters for data in each directory\n",
    "    stacked_bands.append(bands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f599e20a-f227-40be-b01f-853bd979aecd",
   "metadata": {},
   "source": [
    "### Reshaping raster data\n",
    "\n",
    "For each 256 x 256 pixel Sentinel-2 image footprint, we have have created a multiband raster with each band storing reflectance in different wavelengths, computed an NDVI band and appended that to the stack of raster bands, and also stacked bands representing crop type labels for each pixel and a field id indicating which field a pixel belongs to. We have a list of `ndarray` objects of rank 3 with axis 0 for bands, axis 1 for rows, and axis 2 for columns.  \n",
    "\n",
    "Our goal is to create a tabular dataset where each column represents a variable (e.g. crop type, spectral reflectance, or field id) and each row represents a field. This tabular format is what is required for many machine learning tasks. \n",
    "\n",
    "The next stage in our data transformation routine is to reshape the data from an image-style structure (i.e. where variables are stored along the bands or 0 axis) to a tabular style format where variables are stored along the columns axis. A tabular style dataset can be represented as a rank 2 `ndarray` (axis 0 for rows and axis 1 for columns). Therefore, we need a reshaping operation that will transform a rank 3 `ndarray` object to a rank 2 `ndarray` object and arrange variables along the 1 axis. \n",
    "\n",
    "NumPy has several tools that can be used for reshaping data. An `ndarray` object has a <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.reshape.html\" target=\"_blank\">`reshape()`</a> method. This method takes in a tuple of integers that describe the shape of the new `ndarray`. For example, let's demonstrate reshaping a `3 x 3` `ndarray` to a `9 x 1` `ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e07b13-047b-4cfc-b1c3-184a7e8c79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3], \n",
    "             [1, 2, 3],\n",
    "             [1, 2, 3]])\n",
    "\n",
    "a_reshaped = a.reshape((9, 1))\n",
    "print(a_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70462985-b7c2-442e-b48c-6f5509542a07",
   "metadata": {},
   "source": [
    "The transpose is another common operation used for reshaping array-like objects. The transpose operation flips the rows and columns of an array. The transpose of a `5 x 4` array is a `4 x 5` array. NumPy `ndarray` objects have a transpose property `T` which returns the transpose of the array. We can demonstrate a few examples of viewing the transpose of an `ndarray` so you can build up an intuition of how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b3aa90-87cd-4a1e-b35e-adbd47a1d82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3, 4], \n",
    "             [1, 2, 3, 4],\n",
    "             [1, 2, 3, 4]])\n",
    "\n",
    "a_transposed = a.T\n",
    "print(\"Note how the 1 valued elements are now aligned along the columns or 1 axis\")\n",
    "print(a_transposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a96d682-88a5-41e5-a958-3ca310111156",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([[3, 3, 3], \n",
    "             [4, 4, 4],\n",
    "             [5, 5, 5]])\n",
    "\n",
    "b_transposed = b.T\n",
    "print(\"Note how the 3 valued elements are now aligned along the rows or 0 axis\")\n",
    "print(b_transposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca901a3-ae40-4000-ac0f-64b6b9f46ad9",
   "metadata": {},
   "source": [
    "We'll need to reshape the `ndarray` object from rank 3 with shape `(bands, rows, cols)`  to a rank 2 array with shape  `(bands, (rows * cols))`. Instead of arranging the elements for each variable (i.e. reflectance values in different wavelengths, crop type, field id) in an image-style format (like a rank 2 array), the reshape operation will transform the `ndarray` so bands remain on the 0-axis but become rows and the the data values will be arranged along the 1 axis (columns). \n",
    "\n",
    "Let's demonstrate this with the first `ndarray` in `stacked_bands`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f875c-b214-4458-8e9d-358f1e1db6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiband_raster = stacked_bands[0]\n",
    "print(f\"the shape of the first multiband raster in stacked_bands is {multiband_raster.shape}\")\n",
    "\n",
    "rows = multiband_raster.shape[1]\n",
    "cols = multiband_raster.shape[2]\n",
    "n_bands = multiband_raster.shape[0]\n",
    "multiband_reshaped = multiband_raster.reshape(n_bands, rows*cols)\n",
    "print(f\"the shape of the reshaped raster is {multiband_reshaped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e29a6c-4e1d-41f1-863e-1e2adca04ab4",
   "metadata": {},
   "source": [
    "Now the we have reshaped a multiband raster to a tabular format, but the variables are arranged down the 0 axis (rows) and we want them arranged as columns. We can use a transpose operation to flip our now reshaped rank 2 `ndarray` to the desired tabular format structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f1e8d7-5bcf-48b4-9f81-7737473d76b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_array = multiband_reshaped.reshape(n_bands, rows*cols).T\n",
    "print(f\"the shape of the transposed array is {tabular_array.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53715505-3954-4dbe-a030-19f12619ea07",
   "metadata": {},
   "source": [
    "Now we know how to transform our rank 3 `ndarray` objects representing multiband rasters to an `ndarray` representing a tabular format, we can extend our routine to perform the reshaping operations after the band stacking operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efc0402-558e-4965-a09e-71378f133b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking bands and then reshaping to tabular format \n",
    "\n",
    "# Sentinel-2 band names \n",
    "s2_bands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "\n",
    "# a list to store numpy ndarray objects representing multiband rasters of Sentinel-2 reflectance data reshaped to tabular format\n",
    "tables = []\n",
    "\n",
    "# loop over image directories\n",
    "for i in image_dirs:\n",
    "    print(f\"stacking Sentinel-2 bands in {i}\")\n",
    "    # a list to store numpy ndarray objects of Sentinel-2 reflectance data for each band\n",
    "    bands = []\n",
    "    \n",
    "    # loop over each band, read in the data from the corresponding GeoTIFF file into an ndarray\n",
    "    for b in s2_bands:\n",
    "        band_path = os.path.join(os.getcwd(), data_path, i, b + \".tif\")\n",
    "        with rasterio.open(band_path) as src:\n",
    "            # append the ndarray storing the Sentinel-2 reflectance data for a band to a list\n",
    "            bands.append(src.read(1))\n",
    "    \n",
    "    # stack all bands in the list to create a multiband raster\n",
    "    bands = np.stack(bands)\n",
    "    \n",
    "    # make NDVI band\n",
    "    red = bands[3,:,:].astype(\"float64\")\n",
    "    nir = bands[7,:,:].astype(\"float64\")\n",
    "    ndvi = (nir-red)/(nir+red)\n",
    "    ndvi = np.expand_dims(ndvi, axis=0) # add a bands axis\n",
    "    bands = np.concatenate((bands, ndvi), axis=0) # stack the ndvi band\n",
    "    \n",
    "    # add field id band\n",
    "    field_id_path = os.path.join(os.getcwd(), data_path, i, \"field/field_ids.tif\")\n",
    "    with rasterio.open(field_id_path) as src:\n",
    "        field_ids = src.read().astype(\"float64\")\n",
    "        field_ids[field_ids == 0] = np.nan\n",
    "        bands = np.concatenate((bands, field_ids), axis=0)\n",
    "    \n",
    "    # add crop type labels band\n",
    "    labels_path = os.path.join(os.getcwd(), data_path, i, \"label/raster_labels.tif\")\n",
    "    with rasterio.open(labels_path) as src:\n",
    "        bands = np.concatenate((bands, src.read()), axis=0)\n",
    "    \n",
    "    # reshape multiband raster to tabular format\n",
    "    rows = bands.shape[1]\n",
    "    cols = bands.shape[2]\n",
    "    n_bands = bands.shape[0]\n",
    "    reshaped = bands.reshape(n_bands, rows*cols)\n",
    "    tabular = reshaped.T\n",
    "    \n",
    "    # append the tabular array to the list storing tables for the data in each directory\n",
    "    tables.append(tabular)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f765e423-0679-41ec-bb91-cfed92890d42",
   "metadata": {},
   "source": [
    "## Attribute data operations\n",
    "\n",
    "### Pandas `DataFrame` and data cleaning\n",
    "\n",
    "We have applied a series of transformation operations to raster data using NumPy `ndarray` objects. We read several raster datasets stored as GeoTIFF objects into a NumPy `ndarray` object initially creating a multiband raster-like object before reshaping to a tabular-like structure. \n",
    "\n",
    "As our data is now in a tabular structure it makes sense to convert it from a NumPy `ndarray` object to Pandas a `DataFrame` object. Pandas `DataFrame` objects, and the Pandas package more generally, are based on NumPy but have been extended and tailored for working with tabular datasets. For example, a NumPy `ndarray` stores data of the same type in an array-like structure (e.g. all elements are integers). A Pandas `DataFrame` can store different type data in different columns (e.g. column 0 is string, column 1 is floating point, etc.), but the values within each column are the same type and each column is typically a `PandasArray` which is based on a NumPy array. Columns in a Pandas `DataFrame` are called `Series` and a `Series` can be objects in your program independent of a `DataFrame`. \n",
    "\n",
    "One way of thinking about the links between NumPy and Pandas is: a `Series` is an array-like sequence of values stored in a `PandasArray` which wraps a NumPy `ndarray`,  and a `DataFrame` is creates a tabular structure by combining one or more `Series`. \n",
    "\n",
    "Operations on Pandas `DataFrame`s also borrow from NumPy's style such as avoiding for-loops; however, they also provide several features and functions geared towards working with tabular datasets. A selection of these features that are relevant to working with tabular data include:\n",
    "\n",
    "* Indexing using column names.\n",
    "* Relational database style operations including key-based joins, conditional filtering and selection of data, and group-by and summarise.\n",
    "* Support for working with time-series. \n",
    "* More tools for handling missing data.\n",
    "\n",
    "Thus, Pandas `DataFrame` objects are useful for many attribute data transorfmation operations.\n",
    "\n",
    "To convert a NumPy `ndarray` to a Pandas `DataFrame` we pass the array into the <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame\" target=\"_blank\">`DataFrame`'s constructor</a> function helpfully named `DataFrame()`. The constructor function expects the NumPy `ndarray` and a list of column labels as arguments. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2a9af6-4d0f-40de-9bad-bc6a6814ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_array_0 = tables[0]\n",
    "print(f\"table_array_0 is of {type(table_array_0)} type with shape {table_array_0.shape}\")\n",
    "\n",
    "# convert ndarray to Pandas DataFrame\n",
    "s2_bands = ['B01', 'B02', 'B03', 'B04','B05', 'B06', 'B07', 'B08','B8A', 'B09', 'B11', 'B12']\n",
    "\n",
    "# create a DataFrame object\n",
    "tmp_df = pd.DataFrame(table_array_0, columns=s2_bands + [\"ndvi\", \"field_id\", \"labels\"])\n",
    "tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9766a602-3355-46dc-8203-b2e12b50c41e",
   "metadata": {},
   "source": [
    "Looking at the `DataFrame` we can clearly see some `NaN` values in the `field_id` column. As each row in this `DataFrame` represents a pixel in the image, rows with `NaN` values in the `field_id` column are pixels which don't have a crop type label. Therefore, we can drop them using the `dropna()` method - this will drop all rows from the `DataFrame` where there is a `NaN` value.\n",
    "\n",
    "Let's inspect some metadata for the `DataFrame` we have created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2051a-13e8-42b1-94bc-98c7781886a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1338fd0c-2ad9-479e-bb8c-50659739ef30",
   "metadata": {},
   "source": [
    "The `DataFrame`'s `info()` method returns a summary of the column's data types, count of non-null data, and the memory usage for the object. We can see that the `field_id` and `labels` columns are float64; however, these columns are storing categorical data so integer numbers would be a more appropriate type. Therefore, we can use the `astype()` method to convert these columns to integer type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f49ea7-9108-4c08-8739-cdf5b171b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = tmp_df.dropna()\n",
    "tmp_df = tmp_df.astype({\"field_id\": \"int32\", \"labels\": \"int32\"})\n",
    "# Check the cast to integer type has worked. \n",
    "# Also, note the reduced memory usage after dropping all the nan rows\n",
    "tmp_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ac0b10-6b93-4675-9a85-e0b1855607c5",
   "metadata": {},
   "source": [
    "### Grouped summaries\n",
    "\n",
    "Another common attribute operation when working with tabular data is performing grouped aggregations and summaries. For example, often we want to compute the mean, median, min, max, or sum of data values within groups in our dataset. This could be to generate summary tables for reporting purposes or an intermediate step in a data transformation workflow. \n",
    "\n",
    "Our goal for this data transformation workflow is to generate average spectral reflectance values in different wavelengths from Sentinel-2 images for each field with a crop type label. So far we have created a table where each row represents one pixel in a 256 x 256 image footprint and we have many pixels per field. We need to compute the average reflectance values for each field. This is a group-by and summarise operation - grouping by field and summarising using the mean. \n",
    "\n",
    "A group-by and summarise operation can be conceptualised as a sequence of split-apply-combine steps <a href=\"https://wesmckinney.com/book/data-aggregation.html#groupby_fundamentals\" target=\"_blank\">McKinney (2022)</a>:\n",
    "\n",
    "* **Split** your dataset into groups.\n",
    "* **Apply** a function to values in each group as a summary.\n",
    "* **Combine** the results of applying the function to each group. \n",
    "\n",
    "For our dataset we need to group-by `field_id` and `labels` (crop type column) and compute the mean of spectral reflectance values within each group. \n",
    "\n",
    "Pandas `DataFrame`s have a `groupby()` method that can take in a list of one or more column names. Calling this method returns a `GroupBy` object that creates groups from your dataset for each of the unique values of the grouping columns and can be used to apply summary operations to each group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d976db-2bc8-4cab-a41c-4c10ce8998c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a group using field id and crop type\n",
    "tmp_df_groups = tmp_df.groupby([\"field_id\", \"labels\"])\n",
    "print(tmp_df_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548e1e45-35fd-41e8-a135-ef91ae6f245a",
   "metadata": {},
   "source": [
    "Finally, we need to apply our summary operations to each group. We can do this by calling a function on the `GroupBy` object. A useful function for data exploration tasks is calling `size()` which tells us the number of observations in each group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630615e-f8c8-4ea4-ba63-9fefe5cfe470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of groups\n",
    "tmp_df_groups.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500e5fc2-e8e0-4df2-aeff-9be7cfa31285",
   "metadata": {},
   "source": [
    "By calling `size()` on the `GroupBy` object we can see that we have one group with a `field_id` of `2595` and a crop type label of `1` (wheat). There are 15 observations in this group. If we want to compute the mean of the spectral reflectance values within each group we can call `mean()` instead of `size()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e78b80-c7b7-4c7a-b255-ac2085f61529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean spectral reflectance values per group\n",
    "tmp_df_groups.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca9b1a8-c390-43ec-88e4-3bbbbf344883",
   "metadata": {},
   "source": [
    "We're now in a position to update our data transformation routine to include converting the `ndarray` objects in a tabular-like structure to Pandas `DataFrames`, data cleaning to drop `NaN` pixels, and computing the mean spectral reflectance values for each `field_id` and crop type `label` combination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce734806-1d34-432a-bcf0-602a28faf9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking bands, reshaping to tabular format, convert to DataFrame, and summarise by field id and crop type \n",
    "\n",
    "# Sentinel-2 band names \n",
    "s2_bands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "\n",
    "# a list to store DataFrames \n",
    "dfs = []\n",
    "\n",
    "# loop over image directories\n",
    "for i in image_dirs:\n",
    "    print(f\"stacking Sentinel-2 bands in {i}\")\n",
    "    # a list to store numpy ndarray objects of Sentinel-2 reflectance data for each band\n",
    "    bands = []\n",
    "    \n",
    "    # loop over each band, read in the data from the corresponding GeoTIFF file into an ndarray\n",
    "    for b in s2_bands:\n",
    "        band_path = os.path.join(os.getcwd(), data_path, i, b + \".tif\")\n",
    "        with rasterio.open(band_path) as src:\n",
    "            # append the ndarray storing the Sentinel-2 reflectance data for a band to a list\n",
    "            bands.append(src.read(1))\n",
    "    \n",
    "    # stack all bands in the list to create a multiband raster\n",
    "    bands = np.stack(bands)\n",
    "    \n",
    "    # make NDVI band\n",
    "    red = bands[3,:,:].astype(\"float64\")\n",
    "    nir = bands[7,:,:].astype(\"float64\")\n",
    "    ndvi = (nir-red)/(nir+red)\n",
    "    ndvi = np.expand_dims(ndvi, axis=0) # add a bands axis\n",
    "    bands = np.concatenate((bands, ndvi), axis=0) # stack the ndvi band\n",
    "    \n",
    "    # add field id band\n",
    "    field_id_path = os.path.join(os.getcwd(), data_path, i, \"field/field_ids.tif\")\n",
    "    with rasterio.open(field_id_path) as src:\n",
    "        field_ids = src.read().astype(\"float64\")\n",
    "        field_ids[field_ids == 0] = np.nan\n",
    "        bands = np.concatenate((bands, field_ids), axis=0)\n",
    "    \n",
    "    # add crop type labels band\n",
    "    labels_path = os.path.join(os.getcwd(), data_path, i, \"label/raster_labels.tif\")\n",
    "    with rasterio.open(labels_path) as src:\n",
    "        bands = np.concatenate((bands, src.read()), axis=0)\n",
    "    \n",
    "    # reshape multiband raster to tabular format\n",
    "    rows = bands.shape[1]\n",
    "    cols = bands.shape[2]\n",
    "    n_bands = bands.shape[0]\n",
    "    reshaped = bands.reshape(n_bands, rows*cols)\n",
    "    tabular = reshaped.T\n",
    "    \n",
    "    ### HERE WE CONVERT TO DATAFRAMES AND DROP NAN VALUES ###\n",
    "    tmp_df = pd.DataFrame(tabular, columns=s2_bands + [\"ndvi\", \"field_id\", \"labels\"])\n",
    "    tmp_df = tmp_df.dropna()\n",
    "    \n",
    "    # append DataFrame to list of DataFrames \n",
    "    dfs.append(tmp_df)\n",
    "\n",
    "### HERE WE COMBINE ALL THE DATAFRAMES FOR EACH 256 x 256 IMAGE FOOTPRINT AND COMPUTE THE MEAN VALUES PER GROUP ###\n",
    "dfs = pd.concat(dfs)\n",
    "dfs = dfs.astype({\"field_id\": \"int32\", \"labels\": \"int32\"})\n",
    "dfs = dfs.groupby([\"field_id\", \"labels\"]).mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb87db6-9156-4328-b50a-c77e17b4f088",
   "metadata": {},
   "source": [
    "Let's quickly inspect the output. We should have one row per `field_id` and crop type `label` group. The `DataFrame` storing the results of this routine are referenced by the variable `dfs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac2b07-625e-490a-96a0-90fc547b0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ec3fe9-77c1-406a-9d3a-1c5772bf14eb",
   "metadata": {},
   "source": [
    "## Raster-vector operations and vector operations\n",
    "\n",
    "We're almost at the stage where we've processed a number of GeoTIFF files stored across many directories into a tabular dataset in a `DataFrame` object ready for machine learning. However, there are two more columns we need to create and append to the `DataFrame`. The first is a `geometry` column recording the centroid of each field. This allows us to keep a record of each field's geographic location. We'll also use this centroid to identify the district (an administrative boundary below the State-level in India) each field is located in. We'll use this geographic information to ensure training and test data are not spatially correlated in the machine learning workflows (but, that is a discussion for later). \n",
    "\n",
    "To compute the centroid for each field we need to perform some raster-vector operations where each raster dataset is converted to a vector dataset. This is called vectorisation and can be achieved using `rasterio`'s <a href=\"https://rasterio.readthedocs.io/en/latest/api/rasterio.features.html#rasterio.features.shapes\" target=\"_blank\">`shapes()`</a> function which returns the shape and value of connected regions in a raster dataset. Pixels belonging the same field in a raster layer should be connected (i.e. their edges touch) and they should have the same value (field id). Thus, applying the `shapes()` function to the raster layer of field ids should return vector polygons for each field outline.\n",
    "\n",
    "To do this we'll need to use the `field_ids.tif` files. Let's quickly inspect these files again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a08053f-a384-4f9a-8d62-dda18ad92ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in image_dirs:\n",
    "    field_id_path = os.path.join(os.getcwd(), data_path, i, \"field/field_ids.tif\")\n",
    "    with rasterio.open(field_id_path) as src:\n",
    "        print(f\"Printing metadata for field_ids.tif in {i}\")\n",
    "        pprint.pprint(src.meta)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bad732-e6e1-4a6b-bbae-06a32c331753",
   "metadata": {},
   "source": [
    "We have printed dictionary objects of the metadata for each of the `field_ids.tif` files. You will notice that not all files have the same coordinate reference system (the `crs` slot; CRS). This will be a problem if we try and create a `geometry` column where each row stores the point centroid for each field. We'll need all the points in our column to be in the same coordinate reference system. So, we'll need to add a step to this process that reprojects all the point centroids to a common CRS. Here, we'll reproject to latitude and longitude or `EPSG:4326`. \n",
    "\n",
    "Let's look at what the `shapes()` function returns for the first `field_ids.tif` file in `image_dirs` (remember `image_dirs` is a list of directories storing GeoTIFF files). \n",
    "\n",
    "The `shapes()` function takes in a NumPy `ndarray` of raster values (generated by `src.read()` which reads the raster values from the GeoTIFF file into a NumPy `ndarray` in memory) and can be converted to a list object. The list object stores a tuple for each shape in the raster data. The first element of the tuple is a dictionary object of coordinates and the type of geometry (e.g. point, line, polygon). The second element of the tuple is the value that corresponds to the geometry. \n",
    "\n",
    "We can see below that we have two elements in our list. The first has a value of 2592 which corresponds to the field id in the raster layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b041d250-642e-47f7-8197-5b7397ffa18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in image_dirs[0:1]:\n",
    "    field_id_path = os.path.join(os.getcwd(), data_path, i, \"field/field_ids.tif\")\n",
    "    with rasterio.open(field_id_path) as src:\n",
    "        # shapes is a generator\n",
    "        shapes = features.shapes(src.read(), transform=src.transform)\n",
    "        \n",
    "        # list of geometry and shape value\n",
    "        field_shapes = list(shapes)\n",
    "        \n",
    "        pprint.pprint(field_shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31020072-feaf-441a-a271-b2dc5b274b26",
   "metadata": {},
   "source": [
    "At this stage, we've converted our raster data to a list of numbers representing coordinates for the shape. We now need to turn this list of coordinates into a geometry object. In Python, geometry objects as <a href=\"https://shapely.readthedocs.io/en/stable/geometry.html\" target=\"_blank\">Shapely</a> `Geometry` objects. The `geometry` column in a GeoPandas `GeoDataFrame` is a `Series` of Shapely `Geometry` objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31193e75-d473-45ee-ab50-cd8b8edd54df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in image_dirs[0:1]:\n",
    "    field_id_path = os.path.join(os.getcwd(), data_path, i, \"field/field_ids.tif\")\n",
    "    with rasterio.open(field_id_path) as src:\n",
    "        # shapes is a generator\n",
    "        shapes = features.shapes(src.read(), transform=src.transform)\n",
    "        \n",
    "        # list of geometry and shape value\n",
    "        field_shapes = list(shapes)\n",
    "        \n",
    "        # create a list of Shapely Geometry objects\n",
    "        geom = []\n",
    "        for s in field_shapes:\n",
    "            geom.append(shapely.geometry.shape(s[0]))\n",
    "        \n",
    "        print(geom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b89daf-83e0-4d01-84b1-8a045f3455da",
   "metadata": {},
   "source": [
    "Next, we compute the centroid for the polygon shape of the field. Computing a centroid is a geometry operation where the shape's geometry is converted from a polygon to a point feature. To efficiently compute the centroid for the shapes returned by `shapes()` we can convert the list of `Geometry` objects to a `GeoSeries` and then use the `GeoSeries` `centroid` attribute to return a `GeoSeries` of polygon centroids. \n",
    "\n",
    "Inspecting this `GeoSeries` should reveal a sequence of point `Geometry` objects have been computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0ce8ed-32ba-4108-9348-73c3f689eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in image_dirs[0:1]:\n",
    "    field_id_path = os.path.join(os.getcwd(), data_path, i, \"field/field_ids.tif\")\n",
    "    with rasterio.open(field_id_path) as src:\n",
    "        # shapes is a generator\n",
    "        shapes = features.shapes(src.read(), transform=src.transform)\n",
    "        \n",
    "        # list of geometry and shape value\n",
    "        field_shapes = list(shapes)\n",
    "        \n",
    "        # create a list of Shapely Geometry objects\n",
    "        geom = []\n",
    "        for s in field_shapes:\n",
    "            geom.append(shapely.geometry.shape(s[0]))\n",
    "        \n",
    "        # compute centroids\n",
    "        geom = gpd.GeoSeries(geom, crs=src.crs).centroid\n",
    "        \n",
    "        print(geom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed9839-cf8b-424d-8f0f-4c3fa43e81f8",
   "metadata": {},
   "source": [
    "Now we have computed the centroid for each field, we can convert the points to a common coordinate reference system (`EPSG:4326`) using GeoPandas `to_crs()` method. We'll also create another `Series` of field ids and combine the `Series` of field ids with the `GeoSeries` of field centroids to create a `GeoDataFrame`. We'll also drop shapes with the value of 0 as these shapes don't have a crop type label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6df266-86c5-4306-ae03-483182a0c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in image_dirs[0:1]:\n",
    "    field_id_path = os.path.join(os.getcwd(), data_path, i, \"field/field_ids.tif\")\n",
    "    with rasterio.open(field_id_path) as src:\n",
    "        # shapes is a generator\n",
    "        shapes = features.shapes(src.read(), transform=src.transform)\n",
    "        \n",
    "        # list of geometry and shape value\n",
    "        field_shapes = list(shapes)\n",
    "        \n",
    "        # create a list of Shapely Geometry objects\n",
    "        geom = []\n",
    "        for s in field_shapes:\n",
    "            geom.append(shapely.geometry.shape(s[0]))\n",
    "        \n",
    "        # compute centroids\n",
    "        geom = gpd.GeoSeries(geom, crs=src.crs).centroid\n",
    "        \n",
    "        # reproject to EPSG 4326\n",
    "        geom = geom.to_crs(\"EPSG:4326\")\n",
    "        \n",
    "        # create a Series of field ids\n",
    "        field_ids = []\n",
    "        for f in field_shapes:\n",
    "            field_ids.append(f[1])\n",
    "\n",
    "        field_ids = pd.Series(field_ids).astype(\"int32\")\n",
    "        \n",
    "        # Combine the Series and GeoSeries into a DataFrame\n",
    "        field_id_df = gpd.GeoDataFrame({'field_id': field_ids, 'geometry': geom})\n",
    "        \n",
    "        # drop shapes with value 0\n",
    "        field_id_df = field_id_df.loc[field_id_df[\"field_id\"] > 0, :]\n",
    "        \n",
    "        print(field_id_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e4c537-d6ee-4a89-a176-3dfdd30f68dd",
   "metadata": {},
   "source": [
    "Now, we're able to create a small routine that will loop over our list of directories and create a `GeoDataFrame` with a column for the field id and a column for the field centroid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679f6d11-0364-47e9-a22d-c6c054b25d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_id_dfs = []\n",
    "\n",
    "for i in image_dirs:\n",
    "    field_id_path = os.path.join(os.getcwd(), data_path, i, \"field/field_ids.tif\")\n",
    "    with rasterio.open(field_id_path) as src:\n",
    "        # shapes is a generator\n",
    "        shapes = features.shapes(src.read(), transform=src.transform)\n",
    "        \n",
    "        # list of geometry and shape value\n",
    "        field_shapes = list(shapes)\n",
    "        \n",
    "        # create a list of Shapely Geometry objects\n",
    "        geom = []\n",
    "        for s in field_shapes:\n",
    "            geom.append(shapely.geometry.shape(s[0]))\n",
    "        \n",
    "        # compute centroids\n",
    "        geom = gpd.GeoSeries(geom, crs=src.crs).centroid\n",
    "        \n",
    "        # reproject to EPSG 4326\n",
    "        geom = geom.to_crs(\"EPSG:4326\")\n",
    "        \n",
    "        # create a Series of field ids\n",
    "        field_ids = []\n",
    "        for f in field_shapes:\n",
    "            field_ids.append(f[1])\n",
    "\n",
    "        field_ids = pd.Series(field_ids).astype(\"int32\")\n",
    "        \n",
    "        # Combine the Series and GeoSeries into a DataFrame\n",
    "        field_id_df = gpd.GeoDataFrame({'field_id': field_ids, 'geometry': geom})\n",
    "        \n",
    "        # drop shapes with value 0\n",
    "        field_id_df = field_id_df.loc[field_id_df[\"field_id\"] > 0, :]\n",
    "        \n",
    "        field_id_dfs.append(field_id_df)\n",
    "\n",
    "field_id_dfs = pd.concat(field_id_dfs)\n",
    "field_id_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a490b7-6b2e-4e73-a0e3-409d087da259",
   "metadata": {},
   "source": [
    "## Joins\n",
    "\n",
    "### Key-based joins\n",
    "\n",
    "We now have two separate data objects in our Python program. We have a `DataFrame` storing average spectral reflectance values for each field, field id, and crop type label attributes. We also have a `GeoDataFrame` storing the field id attribute and the field centroid as a point `Geometry`. \n",
    "\n",
    "When two tables have a matching column(s) we can use join operations to combine them. Rows in both tables are matched using common values in the matching column(s) and the joined table has columns from both tables. \n",
    "\n",
    "Joining tables is a common operation in relational databases using SQL and the same operations can be implemented in Pandas using <a href=\"https://pandas.pydata.org/docs/user_guide/merging.html#database-style-dataframe-or-named-series-joining-merging\" target=\"_blank\">`merge()`</a> functions. \n",
    "\n",
    "Some important concepts for join operations:\n",
    "\n",
    "* The columns with values used to match rows are called often called **keys**.\n",
    "* **one-to-one** joins are where there is exactly one match between rows in the two tables being joined.\n",
    "* **many-to-one** joins are where a row in one table can match one or more rows in another table.\n",
    "* **left joins** keep all rows in the left table and only matching rows in the right table. \n",
    "* **inner joins** keep only matching rows in the left and right tables. \n",
    "\n",
    "\n",
    "The Pandas <a href=\"https://pandas.pydata.org/docs/user_guide/merging.html#database-style-dataframe-or-named-series-joining-merging\" target=\"_blank\">`merge()`</a> docs and <a href=\"https://wesmckinney.com/book/data-wrangling.html#prep_merge_join\" target=\"_blank\">McKinney (2022)</a> provide useful explanations for how join operations work.\n",
    "\n",
    "Let's use consider these concepts in the context of joining our `DataFrame` `dfs` storing average spectral reflectance values and crop type labels and our `GeoDataFrame` `field_id_dfs` which stores the field centroids. \n",
    "\n",
    "The matching column in both tables is `field_id`. This the joining key. \n",
    "\n",
    "We are joining the two tables on `field_id` which should be unique to each field. Therefore, we are implementing a one-to-one join. \n",
    "\n",
    "As we're using the field centroids for subsequent operations, we only want to keep fields that have a centroid value. Therefore, we'll use an inner join.\n",
    "\n",
    "Pandas `merge()` function expects the following arguments:\n",
    "\n",
    "* `left` - left table in the join.\n",
    "* `right` - right table in the join.\n",
    "* `how` - whether to use a left or inner join.\n",
    "* `left_on` - columns in left table to use as keys.\n",
    "* `right_on` - columns in the right table to use as keys.\n",
    "\n",
    "Let's join the two tables and inspect the result. If the join is successful you should see a `geometry` column appended to the columns in `dfs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c8b92d-8b99-4676-a7a1-0ca678214688",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = pd.merge(left=dfs, right=field_id_dfs, how=\"inner\", left_on=[\"field_id\"], right_on=[\"field_id\"])\n",
    "# display on the first few rows\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b714012-0809-4438-a364-3cfe629e4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert joined_df to GeoDataFrame\n",
    "joined_df = gpd.GeoDataFrame(joined_df, geometry=joined_df.geometry, crs=\"EPSG:4326\")\n",
    "type(joined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d688e82-9f0a-4d5e-b42e-0b8991d675d6",
   "metadata": {},
   "source": [
    "### Spatial Joins\n",
    "\n",
    "Spatial join operations join the attributes of two vector layers based on their relationship in space. For example, if we have a `GeoDataFrame` storing field boundaries (polygon geometries) and field attributes and another `GeoDataFrame` storing shire boundaries (polygon geometries) and a shire name as an attribute, we can join the the two tables based on the largest intersection (overlap) between field boundaries and shire boundaries. If the field boundaries `GeoDataFrame` was the left table in the spatial join, for each row (or geometry feature) the shire name from the shire with largest intersection would be joined to that table in a new column. \n",
    "\n",
    "GeoPandas provides an `sjoin()` function that can be used for spatial joins of two `GeoDataFrames`. The `sjoin()` function expects the following as arguments:\n",
    "\n",
    "* `left_df` - left `GeoDataFrame` in the spatial join.\n",
    "* `right_df` - right `GeoDataFrame` in the spatial join - columns from the `right_df` will be joined to `left_df`.\n",
    "* `how` - whether to use a left, inner, or right join.\n",
    "* `predicate` - a binary predicate that defines the spatial relationship between features in `right_df` and `left_df`. \n",
    "\n",
    "Binary predicates that can be used are:\n",
    "\n",
    "* intersects\n",
    "* contains\n",
    "* crosses\n",
    "* within\n",
    "* touches\n",
    "* overlaps\n",
    "\n",
    "Intersects is the default predicate for spatial joins in GeoPandas. \n",
    "\n",
    "To complete our data transformation routine we need to add a column to `joined_df` that stores the District that the field is located in. We can do this using a spatial join based on the intersect of the field's centroid (point geometry) and the shape of the District (polygon geometry). \n",
    "\n",
    "But, we need to read in District geometries for India obtained from <a href=\"https://www.geoboundaries.org\" target=\"_blank\">geoBoundaries</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4bc54-6936-4034-b4ec-1096ae267bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "india_districts = gpd.read_file(os.path.join(os.getcwd(), \"week-4\", \"india-adm\", \"geoBoundaries-IND-ADM2_simplified.topojson\"))\n",
    "india_districts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d046e3-d8b7-4610-acd1-35e0d3031e7a",
   "metadata": {},
   "source": [
    "Let's quickly tidy up the India Districts `GeoDataFrame` to keep only the District name and `geometry` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb0854e-7104-48a5-bb3f-91e39e32521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "india_districts = india_districts.loc[:, [\"shapeName\", \"geometry\"]]\n",
    "india_districts.columns = [\"district\", \"geometry\"]\n",
    "india_districts = india_districts.set_crs(\"EPSG:4326\")\n",
    "india_districts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff909b-700d-4eb3-929e-d8d53700c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "india_districts.plot(column=\"district\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce980c-4f2f-4d9d-b826-0ef66a40f768",
   "metadata": {},
   "source": [
    "That looks like India. Let's implement our final data transformation step and perform a spatial join to add a District column to `joined_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0245502b-8be1-4977-b6a8-75499c8ce106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join\n",
    "joined_df_district = gpd.sjoin(\n",
    "    left_df=joined_df, \n",
    "    right_df=india_districts, \n",
    "    how=\"inner\", \n",
    "    predicate=\"intersects\"\n",
    ")\n",
    "joined_df_district.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6aedf9-341f-489f-b919-09ccbff05499",
   "metadata": {},
   "source": [
    "### Save file\n",
    "\n",
    "Finally, let's write our processed data ready for training and testing a machine learning model to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b82a6b-e0cc-4a27-9dcf-63928fad8710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "out_path = os.path.join(os.getcwd(), \"week-4\", \"processed_data.geojson\")\n",
    "joined_df_district.to_file(out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
