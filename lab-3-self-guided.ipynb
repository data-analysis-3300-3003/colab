{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f99d780-73b9-4599-900f-1c6fbde4b616",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63254a69-9c6a-4510-a551-3a9060d437a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploratory data analysis\n",
    "\n",
    "Exploratory data analysis is an activity where you ....... *explore your data*. It's often conducted towards the beginning of a data science or analysis workflow and is an interactive process where you build up your familiarity with the data; identify its structure and patterns; spot noise, errors, and missing values; and begin to formulate research questions and hypotheses.  \n",
    "\n",
    "Chapter 7 of R for Data Science <a href=\"https://r4ds.had.co.nz/exploratory-data-analysis.html\" target=\"_blank\">(Wickham and Grolemund, 2017)</a> provides an excellant overview of techniques for exploratory data analysis. They suggest that two questions should guide your initial exploration of datasets:\n",
    "\n",
    "* What type of variation occurs within variables?\n",
    "* What type of covariation occurs between variables?\n",
    "\n",
    "A variable is a property or feature of interest that can be measured and a value is the state of the variable when it was measured. Columns in a `DataFrame` or `GeoDataFrame` or bands in raster often correspond to variables and cells in a table or pixels in a raster correspond to values for an observation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96244668-11dd-467a-b73d-e95c774a4781",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Run the labs\n",
    "\n",
    "You can run the labs locally on your machine or you can use cloud environments provided by Google Colab or Binderhub. **If you're working with Google Colab and Binderhub be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.**\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/data-analysis-3300-3003/colab/blob/main/lab-3-self-guided.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "<a href=\"https://mybinder.org/v2/gh/binder-3300-3003/binder/HEAD\" target=\"_blank\">\n",
    "  <img src=\"https://mybinder.org/badge_logo.svg\" alt=\"Open In Binder\"/>\n",
    "</a>\n",
    "\n",
    "### Download data\n",
    "\n",
    "If you need to download the data for this lab, run the following code snippet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0721da-91c0-485f-8c76-1fe26cc91e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"week-3\" not in os.listdir(os.getcwd()):\n",
    "    os.system('wget \"https://github.com/data-analysis-3300-3003/data/raw/main/data/week-3.zip\"')\n",
    "    os.system('unzip \"week-3.zip\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a9f398-37ef-42cf-a344-aee4f8eabde4",
   "metadata": {},
   "source": [
    "### Working in Colab\n",
    "\n",
    "If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be90d5ea-91bc-4fe9-a396-00d4e4e10711",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install geopandas\n",
    "    !pip install pyarrow\n",
    "    !pip install mapclassify\n",
    "    !pip install rasterio\n",
    "    !pip install libpysal\n",
    "    !pip install esda\n",
    "    !pip install splot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a91b416-8469-4a0c-938a-160ea1cdfe2c",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a992cb-a166-441d-a313-4e218396a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import plotly.io as pio\n",
    "import esda\n",
    "\n",
    "from splot.esda import plot_moran\n",
    "from libpysal.weights import KNN, lag_spatial\n",
    "\n",
    "# setup renderer\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    pio.renderers.default = \"colab\"\n",
    "else:\n",
    "    pio.renderers.default = \"jupyterlab\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b0f5b-16d6-4874-8c7d-eb5b6bad9fe5",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb674bae-07ca-420e-b6c1-7109cf092fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the crop yield data\n",
    "crop_yield_data_path = os.path.join(os.getcwd(), \"week-3\")\n",
    "\n",
    "# Get a list of crop yield data\n",
    "crop_yield_data_files = os.listdir(crop_yield_data_path)\n",
    "\n",
    "# Combine the geojson files into one GeoDataFrame\n",
    "dfs = []\n",
    "\n",
    "for i in crop_yield_data_files:\n",
    "    if i.endswith(\".geojson\"):\n",
    "        print(f\"Loading file {i} into a Geopandas GeoDataFrame\")\n",
    "        tmp_df = gpd.read_file(os.path.join(crop_yield_data_path, i))\n",
    "        dfs.append(tmp_df)\n",
    "\n",
    "gdf = pd.concat(dfs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2de0116-5d5f-4e58-ad18-ad9063f1512b",
   "metadata": {},
   "source": [
    "## Data summaries\n",
    "\n",
    "An initial data exploration task is to produce summary statistics for the variables in our datasets. Pandas `DataFrame`s and GeoPandas `GeoDataFrame`s have a `describe()` method which generates a `DataFrame` of summary statistics for each variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66aecec-9ae1-458a-91b1-697199352fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe our DataFrame of crop yield data\n",
    "gdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c8c036-a285-4d95-9ff2-6d95cc86c5a0",
   "metadata": {},
   "source": [
    "`describe()` returns to us a count of the number of observations in a variable, the mean value for observations in a variable, and summary statistics describing the distribution and range of values (standard deviation, percentiles (median = 50th percentile), and min and max values).\n",
    "\n",
    "However, there are two main groups in our dataset: canola observations and wheat observations denoted by the `Variety` column. It will be more informative to generate summary statistcs for each group separately. We can do this using the Pandas `groupby()` function which splits a `DataFrame` into subsets based upon a grouping variable, computes statistics for each subset, and then combines the results. Here, we need to `groupby()` `Variety` to generate summary statistics for each crop type. \n",
    "\n",
    "We'll also generate these summary statistics within a context manager (denoted by a `with` block). This context allows us to change the default display values for a `DataFrame` only for this context without affecting the global defaults that apply to the rest of the notebook. This is a useful trick in case you have a particular need to control how a `DataFrame` is displayed (e.g. printing all rows as specified by the `display.max_rows` option)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8907c60d-4f8f-4407-a132-d6404f446bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_rows\", None, \"display.float_format\", lambda x: \"%.3f\" % x):\n",
    "    display(gdf.groupby([\"Variety\"]).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f52aa7-b6a8-4a22-aac8-3d297ce9a610",
   "metadata": {},
   "source": [
    "This still isn't a very helpful layout to view the summary statistics as not all of the statistics can be displayed. Let's transpose the summary statistics so rows become columns and vice versa using the `T` transpose operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c859a2-5d02-4c1f-8514-694838b98488",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_rows\", None, \"display.float_format\", lambda x: \"%.3f\" % x):\n",
    "    display(gdf.loc[:, [\"Variety\", \"DryYield\", \"gndvi\", \"ndyi\"]].groupby([\"Variety\"]).describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc6e979-8c49-41c8-bbad-7813c0b22391",
   "metadata": {},
   "source": [
    "## Data distributions\n",
    "\n",
    "The mean tells us the average value for a variable. However, it is susceptible to outliers and extreme values. Therefore, it is important to view the mean and the median (50th percentile) together as the median is not affected by extreme values. \n",
    "\n",
    "However, neither the mean or the median reveal the spread or distribution of values for a variable. The min and max values tell us what the range of values are for a variable. This can be useful for detecting potential measurement error and noise (e.g. is the max value for wheat yield sensible?).\n",
    "\n",
    "<details>\n",
    "    <summary><b>What is a limitation of using min and max values to represent the distribition of values for a variable?</b></summary>\n",
    "The min and max values can be affected by extreme values and don't tell us anything about the shape or density of the distribution of data values.\n",
    "</details>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "The inter-quartile range (difference between the 75th and 25th percentile values) tells us how spread out the data is around the median and the standard deviation tells us how spread out the data is around the mean. Assuming a normal distribution, ~68% of the values are within one standard deviation of the mean. \n",
    "\n",
    "<details>\n",
    "    <summary><b>If the standard deviation is small relative to the mean, what does this tell us about the spread of the data?</b></summary>\n",
    "There is not much spread in the data away from the mean.\n",
    "</details>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "It is often useful to visualise the distribution of variables. A histogram is a common visualisation for distributions. The height of the bars of a histogram correspond to the count of values that fall within the bin. The width of the bar corresponds to the bin width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b92b3c3-5505-4bb2-b319-53d38a93332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data_frame=gdf, \n",
    "    x=\"DryYield\", \n",
    "    facet_col=\"Variety\", \n",
    "    hover_data=[\"DryYield\", \"Elevation\", \"WetMass\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9374d-2deb-4af9-9a21-b17714c97080",
   "metadata": {},
   "source": [
    "The `histogram()` function from Plotly Express has a `nbins` parameter that can be used to specify the number of bins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a7535d-7098-42c9-a168-a722c6cdded9",
   "metadata": {},
   "source": [
    "### Recap quiz\n",
    "\n",
    "**Use the Plotly Express `nbins` parameter of the `histogram()` function to change the number bins, and consequently the bin width, and explore how this affects the visualisation of the distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64824f8d-58b9-4dbd-8b8a-85940d0d34a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be1554c-1d5c-4a21-a9b3-7ed862799bd3",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "    \n",
    "```{python}\n",
    "fig = px.histogram(\n",
    "    data_frame=gdf, \n",
    "    x=\"DryYield\", \n",
    "    facet_col=\"Variety\", \n",
    "    nbins=5, ### CHANGE THIS VALUE\n",
    "    hover_data=[\"DryYield\", \"Elevation\", \"WetMass\"])\n",
    "fig.show()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098918e3-c843-45f5-af7b-cc2944a68129",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>What happens if you generate a histogram with too large a bin width?</b></summary>\n",
    "You will smooth out variation in the data and will not accurately reflect the shape of the distribution of the data.\n",
    "</details>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<details>\n",
    "    <summary><b>What happens if you generate a histogram with too small a bin width?</b></summary>\n",
    "You might not be able to see the dominant shape of the distribution of data values as the histogram could appear \"spikey\" with local variability in the distribution of values emphasised.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e752d7f-9325-4561-8d80-72979d8e2b9b",
   "metadata": {},
   "source": [
    "## Subsetting pandas `DataFrame`s\n",
    "\n",
    "To subset data from a pandas `DataFrame` we use the square brackets `[]` to specify the data values we'd like to extract. The `[]` operator can be thought of as a get item operation. \n",
    "\n",
    "**Selection by labels**\n",
    "\n",
    "*Selection by labels* refers to selecting values from a `DataFrame` by their label (i.e. column name).\n",
    "\n",
    "* To select a column from a `DataFrame` we pass the column name into `[]` (e.g. `dry_yield = gdf[\"DryYield\"]` where `dry_yield` is a `Series` object).\n",
    "* To select many columns from a `DataFrame` we pass a list of column names into `[]` (e.g. (e.g. `df_yield = gdf[[\"DryYield\", \"Variety\"]]` where `df_yield` is a `DataFrame` object).\n",
    "\n",
    "**Selection by position**\n",
    "\n",
    "*Selection by position* refers to selecting values from a `DataFrame` by their index position (i.e. row or column number starting at 0).\n",
    "\n",
    "* To select the $n^{th}$ row pass in `[n-1:n]`. Remember that Python indexes from zero so the $n-1$ index position is the $n^{th}$ row. The slice operator `:` is exclusive so `[n-1:n]` will only select the row at `n-` (e.g. to select the $2^{nd}$ row use `df_row_2 = gdf[1:2]`).\n",
    "* To select a slice of rows use the slice operator (e.g. to select the first 10 rows use `df_10_rows = gdf[0:10]`).\n",
    "\n",
    "**Selection by condition**\n",
    "\n",
    "*Selection by condition* selects rows that are `True` based on a condition (e.g. selecting all rows with a `DryYield` greater than 1.5 - `df_yield_gt_1_5 = gdf[gdf[\"DryYield\"] > 1.5]`).\n",
    "\n",
    "**`loc[]` and `iloc[]`**\n",
    "\n",
    "The more robust approach to subsetting data from `DataFrame`s is using the `loc` and `iloc` methods, which also support multi-index selection (i.e. selecting rows and columns). \n",
    "\n",
    "* `loc` is used for selecting by labels (e.g. `df_yield = gdf.loc[:, [\"DryYield\", \"Variety\"]]` - note the `[:, [\"DryYield\", \"Variety\"]]` syntax where `:` means select all rows).\n",
    "* `iloc` is used for selecting by position (e.g. to select the first 10 rows use `df_10_rows = gdf.iloc[0:10, :]`).\n",
    "* To select the first 10 rows and columns we'd use `df_10_rows_10_cols = gdf.iloc[0:10, 0:10]`).\n",
    "\n",
    "### Recap quiz\n",
    "\n",
    "<details>\n",
    "    <summary><b>How many rows will the object referenced by <code>df_temp</code> have after calling <code>df_temp=df.iloc[0:5, :]</code>?</b></summary>\n",
    "5 rows at index positions 0 to 4 from the <code>DataFrame</code> <code>df</code>.\n",
    "</details>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<details>\n",
    "    <summary><b>How many columns from <code>df</code> will the object referenced by <code>df_temp</code> have after calling <code>df_temp=df.iloc[0:5, :]</code>?</b></summary>\n",
    "All the columns from <code>df</code>.\n",
    "</details>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "**Use `loc` and the `DataFrame` referenced by `gdf` to select all rows where `DryYield` is greater than 2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af762c12-4a99-4389-9c49-fd6176220aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f16a0-b590-4d79-a33f-7d17894291fc",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```{python}\n",
    "df_gt_2 = gdf.loc[(gdf[\"DryYield\"] > 2), :]\n",
    "df_gt_2.head()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085e8ed0-6b7a-470c-a46f-797835952c29",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "The majority of data values on the histograms above are concentrated on the far left of the figure. If you zoom in you will see there are a few isolated extreme or outlier yield values, which are masking the dominant pattern of the distribution. Detecting outliers is an important part of exploratory data analysis. \n",
    "\n",
    "Now that outliers have been detected we need to fix or remove them. A common way to detect outliers is to use a threshold based on percentile or standard deviation values. Here, we'll say an outlier is any value that is more or less than three standard deviations from the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f881c92a-2e70-4d4c-aa36-84bb3090d2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canola\n",
    "df_canola = gdf.loc[gdf[\"Variety\"] == \"43Y23 RR\", :]\n",
    "print(f\"There are {df_canola.shape[0]} canola rows BEFORE dropping outliers\")\n",
    "df_canola = df_canola.loc[(df_canola[\"DryYield\"]-df_canola[\"DryYield\"].mean()).abs() < (3*df_canola[\"DryYield\"].std()), :]\n",
    "print(f\"There are {df_canola.shape[0]} canola rows AFTER dropping outliers\")\n",
    "\n",
    "# Wheat\n",
    "df_wheat = gdf.loc[gdf[\"Variety\"] == \"Ninja\", :]\n",
    "print(f\"There are {df_wheat.shape[0]} wheat rows BEFORE dropping outliers\")\n",
    "df_wheat = df_wheat.loc[(df_wheat[\"DryYield\"]-df_wheat[\"DryYield\"].mean()).abs() < (3*df_wheat[\"DryYield\"].std()), :]\n",
    "print(f\"There are {df_wheat.shape[0]} wheat rows AFTER dropping outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a33e9-5781-4863-a5de-db97fc53619b",
   "metadata": {},
   "source": [
    "### Recap quiz\n",
    "\n",
    "<details>\n",
    "    <summary><b>Is <code>df_canola = gdf.loc[gdf[\"Variety\"] == \"43Y23 RR\", :]</code> and example of <em>selection by condition</em> or <em>selection by position</em>?</b></summary>\n",
    "Selection by condition. Here, we're using a logical condition that evaluates to <code>True</code> for all rows where the value in the <code>Variety</code> column is <code>43Y23 RR</code>.\n",
    "</details>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<details>\n",
    "    <summary><b>What is the <code>abs()</code> function being used for in the conditional selection of rows whose <code>DryYield</code> value is more / less than three standard deviations from the mean?</b></summary>\n",
    "<code>abs()</code> returns the absolute numeric value (so -2 would be returned as 2). This converts all negative deviations from the mean to positive and allows us to test for rows less than 3 standard deviations from the mean and select all rows within three standard deviations of the mean. \n",
    "</details>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<details>\n",
    "    <summary><b>What is the pandas <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.concat.html\" target=\"_blank\"><code>concat()</code></a> function used for?</b></summary>\n",
    "To combine (concatenate) `DataFrames` along an axis. Here, we specify the 0 axis which refers to rows to stack two `DataFrame`s on top of each other.  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f01df53-89e0-462a-9ecf-336ec00551c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine filtered dfs\n",
    "gdf_clean = pd.concat([df_canola, df_wheat], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6158e-05e4-4a07-bdd2-ce46832409e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data_frame=gdf_clean, \n",
    "    x=\"DryYield\", \n",
    "    facet_col=\"Variety\", \n",
    "    marginal=\"box\", \n",
    "    hover_data=[\"DryYield\", \"Elevation\", \"WetMass\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f3baca-6172-416e-bde5-5db85fcf13e0",
   "metadata": {},
   "source": [
    "Instead of dropping rows where there are extreme crop yield values, we can also replace outlier values with a more sensible value such as the mean.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc441774-30f0-4d74-92ee-d4ecdda714f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_yield = gdf.loc[:, [\"Variety\", \"DryYield\"]].groupby([\"Variety\"]).mean()\n",
    "mean_yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d584f0-f3f1-4066-8fb3-6697fdf73bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_canola = gdf.loc[gdf[\"Variety\"] == \"43Y23 RR\", :]\n",
    "df_canola.loc[(df_canola[\"DryYield\"]-df_canola[\"DryYield\"].mean()).abs() > (3*df_canola[\"DryYield\"].std()), \"DryYield\"] = mean_yield.iloc[0, 0]\n",
    "\n",
    "df_wheat = gdf.loc[gdf[\"Variety\"] == \"Ninja\", :]\n",
    "df_wheat.loc[(df_wheat[\"DryYield\"]-df_wheat[\"DryYield\"].mean()).abs() > (3*df_wheat[\"DryYield\"].std()), \"DryYield\"] = mean_yield.iloc[1, 0]\n",
    "\n",
    "# combine filtered dfs\n",
    "gdf_replaced = pd.concat([df_canola, df_wheat], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211f5b0-246f-4abe-9383-5cca6a91658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data_frame=gdf_replaced, \n",
    "    x=\"DryYield\", \n",
    "    facet_col=\"Variety\", \n",
    "    marginal=\"box\", \n",
    "    hover_data=[\"DryYield\", \"Elevation\", \"WetMass\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53126590-bf57-4d24-a0b4-4edb8424e347",
   "metadata": {},
   "source": [
    "Looking at the wheat yield histogram we can see there are a large number of zero or close to zero values. This is another strange artefact in the distribution of our data values. Are zero crop yield values actually no crop yield from the plant or a source of measurement error or other noise? If the latter, we should remove these noisy values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b0bed-b7dc-4bbe-9000-68042b9140b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_canola = gdf_clean.loc[gdf_clean[\"Variety\"] == \"43Y23 RR\", :]\n",
    "print(f\"The number of canola observations with yield values of zero or less is: {(df_canola['DryYield'] <= 0).sum()}\")\n",
    "df_wheat = gdf_clean.loc[gdf_clean[\"Variety\"] == \"Ninja\", :]\n",
    "print(f\"The number of wheat observations with yield values of zero or less is: {(df_wheat['DryYield'] <= 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb825152-34c1-4879-a8a0-4d301cc765cf",
   "metadata": {},
   "source": [
    "Above, when setting the outlier values to their mean value, we demonstrated how can use `loc` on the left hand side of an expression to determine which values are set. \n",
    "\n",
    "Here, all rows in `df_canola` with a value more / less than three standard deviations from the mean are set to the mean value.\n",
    "\n",
    "`df_canola.loc[(df_canola[\"DryYield\"]-df_canola[\"DryYield\"].mean()).abs() > (3*df_canola[\"DryYield\"].std()), \"DryYield\"] = mean_yield.iloc[0, 0]`\n",
    "\n",
    "### Recap quiz\n",
    "\n",
    "**We're going to drop zero `DryYield` values as these values are noisy. First, we'll set all zero `DryYield` values to `nan` and then we'll use the pandas `dropna()` method to drop all rows with `nan` values in the `DryYield` column.**\n",
    "\n",
    "**Can you edit the call to `loc` below to ensure only zero values in the `DryYield` column are set to zero?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a129187-56a8-4d9a-8163-387c60f09b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EDIT THIS STATEMENT ##\n",
    "df_canola.loc[] = np.nan\n",
    "\n",
    "## EDIT THIS STATEMENT ## \n",
    "df_wheat.loc[] = np.nan\n",
    "\n",
    "# combine filtered dfs\n",
    "gdf_with_nan = pd.concat([df_canola, df_wheat], axis=0)\n",
    "\n",
    "# drop NAs\n",
    "gdf_dropped_nan = gdf_with_nan.dropna(subset=[\"DryYield\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8813137-2457-408d-809f-f64441c13795",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```{python}\n",
    "df_canola.loc[df_canola[\"DryYield\"] <= 0, \"DryYield\"] = np.nan\n",
    "df_wheat.loc[df_wheat[\"DryYield\"] <= 0, \"DryYield\"] = np.nan\n",
    "\n",
    "# combine filtered dfs\n",
    "gdf_with_nan = pd.concat([df_canola, df_wheat], axis=0)\n",
    "\n",
    "# drop NAs\n",
    "gdf_dropped_nan = gdf_with_nan.dropna(subset=[\"DryYield\"])\n",
    "```\n",
    "</details>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "**Can you generate a histogram visualising the distribution of `DryYield` values after dropping zero values? Use the `Variety` column to generate faceted subplots for each wheat and canola.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba259e2-0878-4af9-b8b9-ee88593c6010",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8f487d-eef0-4215-a5fb-0b38d297d3f2",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```{python}\n",
    "fig = px.histogram(\n",
    "    data_frame=gdf_dropped_nan, \n",
    "    x=\"DryYield\", \n",
    "    facet_col=\"Variety\", \n",
    "    marginal=\"box\", \n",
    "    hover_data=[\"DryYield\", \"Elevation\", \"WetMass\"])\n",
    "fig.show()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b55652-a1d9-411c-83b4-8c9d19f791bf",
   "metadata": {},
   "source": [
    "After removing zero values the distribution of our crop yield values should look more sensible and relatively normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548521f8-21e1-42e1-aa73-0e34babc6f8b",
   "metadata": {},
   "source": [
    "### 2D histograms\n",
    "\n",
    "We can use 2D histograms or density heatmaps to look at the distribution of two variables together. 2D hisograms are a useful complement to scatter plots when you have a large number of observations. Here, colour is used to represent the distribution of data values as opposed to the height of rectangular bars on a histogram.\n",
    "\n",
    "Let's create 2D histograms to visualise the relationship between vegetation indices and canola crop yield. Note, you'll need to have completed the recap quiz above to generate `gdf_dropped_nan`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621215a-4996-432a-8a60-c5dc72ff4b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.density_heatmap(\n",
    "    data_frame=gdf_dropped_nan.loc[gdf_dropped_nan[\"Variety\"] == \"43Y23 RR\", :], \n",
    "    x=\"DryYield\", \n",
    "    y=\"gndvi\", \n",
    "    marginal_x=\"box\", \n",
    "    marginal_y=\"box\",\n",
    "    range_y=[0.4, 0.8])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24624ba2-fc09-4a99-bf2d-3b0925913ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.density_heatmap(\n",
    "    data_frame=gdf_dropped_nan.loc[gdf_dropped_nan[\"Variety\"] == \"43Y23 RR\", :], \n",
    "    x=\"DryYield\", \n",
    "    y=\"ndyi\", \n",
    "    marginal_x=\"box\", \n",
    "    marginal_y=\"box\",\n",
    "    range_y=[0.1, 0.5])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077f4f8d-df69-46b4-b38f-ba7bb66ca259",
   "metadata": {},
   "source": [
    "### Violin plots\n",
    "\n",
    "One of the limits of using histograms to visualise distributions is the size of the bins affects the distribution of data values. An alternative approach to visualising a distribution is to use a violin plot. \n",
    "\n",
    "Violin plots use density and box plots to visualise distributions, which look similar to violins. The density is the probability of an observation taking on a certain value and is plotted as a smooth curve. Areas where the curve is fatter indicate a higher probability that an observation will take that value. Box plots display the 25th, 50th, and 75th percentile values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65946cff-9ec5-4245-ba54-b31df6582d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.violin(\n",
    "    gdf_dropped_nan, \n",
    "    y=\"DryYield\", \n",
    "    x=\"Variety\", \n",
    "    color=\"Variety\", \n",
    "    box=True, \n",
    "    points=\"outliers\", \n",
    "    hover_data=[\"DryYield\", \"gndvi\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb02aff3-7b3d-4f61-b776-92515687baeb",
   "metadata": {},
   "source": [
    "### Recap quiz\n",
    "\n",
    "**Can you create violin plots to visualise the GNDVI and NDYI values for each crop type?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad7dcd0-6549-4f3d-84c2-bd3a62a81d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD GNDVI VIOLIN PLOT CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dd0eb8-908e-4582-becc-00639db9c1b3",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "    \n",
    "```{python}\n",
    "fig = px.violin(\n",
    "    gdf_dropped_nan, \n",
    "    y=\"gndvi\", \n",
    "    x=\"Variety\", \n",
    "    color=\"Variety\", \n",
    "    box=True, \n",
    "    points=\"outliers\", \n",
    "    hover_data=[\"DryYield\", \"gndvi\"])\n",
    "fig.show()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc3788-6922-4f7d-a08e-4e9b9be058bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD NDYI VIOLIN PLOT CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2884a90-e131-4499-88db-baaf9727cb30",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "    \n",
    "```{python}\n",
    "fig = px.violin(\n",
    "    gdf_dropped_nan, \n",
    "    y=\"ndyi\", \n",
    "    x=\"Variety\", \n",
    "    color=\"Variety\", \n",
    "    box=True, \n",
    "    points=\"outliers\", \n",
    "    hover_data=[\"DryYield\", \"ndyi\"])\n",
    "fig.show()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bf4951-5ce3-46f8-a981-bb075b3f3510",
   "metadata": {},
   "source": [
    "## Covariation and correlation\n",
    "\n",
    "Variation describes the distribution of values within a variable, covariation describes how values vary between two variables. Postive covariance indicates that as the values of one variable increase so do the values of the other variable (or they both decrease together). Negative covariance indicates that as values of one variable increase the values of the other variable decrease. \n",
    "\n",
    "We can use scatter plots to explore the covariance in our data. We did this above when we looked at the relationships between NDYI and GNDVI and crop yield.\n",
    "\n",
    "We can compute the covariance between two variables as:\n",
    "\n",
    "$$Cov(X,Y)=\\frac{1}{N}\\sum_{i=1}^{n}(X_{i}-\\bar{X})(Y_{i}-\\bar{Y})$$\n",
    "\n",
    "However, a limit of using the covariance to measure association and relationships between variables is that it is affected by the units of measurement. For example, if we had the same crop yield measurements but in units of tonnes per hectare and kilograms per hectare we'd get different covariance scores. Therefore, the correlation coefficient is often used as a measure of association between variables. \n",
    "\n",
    "$$Corr{X,Y} = \\frac{Cov(X,Y)}{sd(X) \\cdot sd(Y)} = \\frac{\\sigma_{XY}}{\\sigma{X}\\sigma{Y}}$$\n",
    "\n",
    "The correlation coefficient is bound between -1 and 1 with -1 indicating perfect negative correlation and 1 indicating perfect positive correlation. \n",
    "\n",
    "Pandas has a <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html\" target=\"_blank\">`corr`</a> function that can be used to compute correlation coefficients between columns in a `DataFrame`. Let's compute the correlation coefficients between crop yields and NDYI and GNDVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e72820e-f42b-4198-becc-b1af3f0963dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_corr = gdf_dropped_nan.loc[:, [\"Variety\", \"DryYield\", \"gndvi\", \"ndyi\"]].groupby([\"Variety\"]).corr()\n",
    "gdf_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1879a73-ce1e-4c3b-a7da-6e3abc4f1c48",
   "metadata": {},
   "source": [
    "### Recap quiz\n",
    "\n",
    "<details>\n",
    "    <summary><b>Why did we call <code>groupby([\"Variety\"])</code> before computing the correlation coefficients?</b></summary>\n",
    "To compute the correlations between vegetation indices (GNDVI and NDYI) and crop yield separately for each crop type. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badc9db6-1add-41b7-a389-7ef02099c776",
   "metadata": {},
   "source": [
    "## Spatial correlation\n",
    "\n",
    "Spatial correlation or spatial autocorrelation describes how similar values are for observations that are close to each other in space. Spatial autocorrelation is the degree to which similar values cluster together in space and is the \"absence of spatial randomness\" <a href=\"https://geographicdata.science/book/notebooks/06_spatial_autocorrelation.html\" target=\"_blank\">(Rey et al. 2020)</a>. Spatial randomness occurs when the values of observations display have no relationship with their location in space.\n",
    "\n",
    "Some key terms related to spatial autocorrelation:\n",
    "\n",
    "* *positive spatial autocorrelation*: the values of nearby locations are similar (e.g. a location and its neighbours both have high or low values).\n",
    "* *negative spatial autocorrelation*: similar values are located far way from each other (less common than spatial autocorrelation). \n",
    "* *global spatial autocorrelation*: a description or summary of the general trend of spatial autocorrelation in the dataset.\n",
    "* *local spatial autocorrelation*: a local measure of how similar an observation's values are to its neighbours which is useful for identifying clusters or similar or disimilar values in a dataset.\n",
    "\n",
    "We can visually explore the spatial correlation in our data by plotting it on a map and using a suitable colour scale to map data values to colours. Let's visualise the canola crop yield values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb453866-6ac2-4541-8948-b306d1c910bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_canola = gdf_dropped_nan.loc[gdf_dropped_nan[\"Variety\"] == \"43Y23 RR\", :].copy()\n",
    "\n",
    "df_canola.explore(column=\"DryYield\", cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90e3b3-b0e2-4e0b-a35e-cb5ab0c7e7d1",
   "metadata": {},
   "source": [
    "Visually, we can see some spatial patterns in our canola yield data. High yield values with yellow / green shades appear to be clustered together in the field. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42efef4-df2e-42bb-a954-b1625cc20bc7",
   "metadata": {},
   "source": [
    "While we can get a general sense of the degree of spatial autocorrelation in our dataset by viewing the data on a map, this does not provide us with a formal method to quantify how spatially correlated the dataset is. \n",
    "\n",
    "The global Moran's I statistic can be used as a measure of spatial autocorrelation in the dataset and for undertaking statistical inference to assess how likely realising the observed pattern of spatial correlation is under a context of spatial randomness. This is analogous to statistical significance / hypothesis testing. \n",
    "\n",
    "The global Moran's I statistic is a measure the correlation between an observation's value and its neighbours. The value of a neighbouring values is termed the spatial lag.\n",
    "\n",
    "A scatter plot can help us build up our intuition of the Moran's I statistic. First, let's compute each canola yield observation's spatial lag (i.e. the average crop yield of it's four nearest neighbours here - there a range of methods for defining neighbouring values).\n",
    "\n",
    "We do this by generating a spatial weights matrix. This is an object that keeps a record of the which data points are neighbours of a given observation in our `GeoDataFrame`. Here, we're defining neighbours using the K nearest neighbour rule; specifically the four nearest neighbours. \n",
    "\n",
    "We row standardise the weights in the spatial weights matrix so each neighbour contributes equal weight and the sum of the neighbour's weights is one. This has the useful effect of the sum of each neighbours weight multiplied by its value returning the average value for all neighbours of an observation: its spatial lag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8cca34-4118-4eeb-93a2-c402d49136d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spatial weights matrix to identify a locations neighbouring values\n",
    "w_knn = KNN.from_dataframe(df_canola, k=4)\n",
    "# Row-standardization\n",
    "w_knn.transform = \"R\"\n",
    "print(f\"the neighbours of the first observation in our dataset are {w_knn.neighbors[0]}\")\n",
    "\n",
    "# compute spatial lag of canola yield\n",
    "df_canola.loc[:, \"DryYield_SpatialLag\"] = lag_spatial(w_knn, df_canola.loc[:, \"DryYield\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7573376-177a-487a-8d36-c48fe102b184",
   "metadata": {},
   "source": [
    "The Moran's plot is a scatter plot that visualises the variable of interest, crop yield here, against its spatial lag. This scatter plot reveals the pattern of spatial autocorrelation in the dataset. If we see a trend of values from the bottom left to top right corners of the graph it indicates there is positive spatial autocorrelation, a trend of values from top left to bottom right indicates negative spatial autocorrelation, and no visible trend indicates spatial randomness. \n",
    "\n",
    "The Moran's plot uses standardised values where the global mean for a variable is subtracted from each observation. \n",
    "\n",
    "The slope of the trend line on the scatter plot is the Moran's I statistic, it indicates the sign and strength of spatial correlation in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92ce0de-9c96-44d0-b198-833671753d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise yield measurements\n",
    "df_canola.loc[:,\"DryYield_std\"] = df_canola.loc[:, \"DryYield\"] - df_canola.loc[:, \"DryYield\"].mean()\n",
    "df_canola[\"DryYield_SpatialLag_std\"] = df_canola.loc[:, \"DryYield_SpatialLag\"] - df_canola.loc[:, \"DryYield_SpatialLag\"].mean()\n",
    "\n",
    "# morans plot\n",
    "fig = px.scatter(\n",
    "    df_canola,\n",
    "    x = \"DryYield_std\",\n",
    "    y = \"DryYield_SpatialLag_std\",\n",
    "    trendline = \"ols\",\n",
    "    opacity=0.05,\n",
    "    labels={\"DryYield_std\": \"Standardised Canola Yield\",\n",
    "           \"DryYield_SpatialLag_std\": \"Standardised Canola Yield (spatial lag)\"}\n",
    ")\n",
    "fig.add_hline(y=0, line_width=0.5)\n",
    "fig.add_vline(x=0, line_width=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6905ed-b2c2-4b16-823f-79114fd9dc36",
   "metadata": {},
   "source": [
    "We can also compute the Moran's I statistic which is a statistical summary of the general level of spatial autocorrelation in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffaed02-14e2-4639-8d1f-e98ac2f6fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "moran = esda.moran.Moran(df_canola[\"DryYield\"], w_knn)\n",
    "print(f\"Moran's I statistic is: {round(moran.I, 3)}\")\n",
    "print(f\"P-value for Moran's I statistic is: {moran.p_sim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4a592-0362-48d7-9efa-b876a93c38ba",
   "metadata": {},
   "source": [
    "This p-value indicates the probability of obtaining a Moran's I statistic this large over 999 simulations with spatial randomness. This indicates that we can reject a null hypothesis of canola yield values being arranged in a spatially random manner in the field. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
