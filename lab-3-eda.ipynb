{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f99d780-73b9-4599-900f-1c6fbde4b616",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63254a69-9c6a-4510-a551-3a9060d437a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploratory data analysis\n",
    "\n",
    "Exploratory data analysis is an activity where you ....... *explore your data*. It's often conducted towards the beginning of a data science or analysis workflow and is an interactive process where you build up your familiarity with the data; identify its structure and patterns; spot noise, errors, and missing values; and begin to formulate research questions and hypotheses.  \n",
    "\n",
    "Chapter 7 of R for Data Science <a href=\"https://r4ds.had.co.nz/exploratory-data-analysis.html\" target=\"_blank\">(Wickham and Grolemund, 2017)</a> provides an excellant overview of techniques for exploratory data analysis. They suggest that two questions should guide your initial exploration of datasets:\n",
    "\n",
    "* What type of variation occurs within variables?\n",
    "* What type of covariation occurs between variables?\n",
    "\n",
    "A variable is a property or feature of interest that can be measured and a value is the state of the variable when it was measured. Columns in a `DataFrame` or `GeoDataFrame` or bands in raster often correspond to variables and cells in a table or pixels in a raster correspond to values for an observation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96244668-11dd-467a-b73d-e95c774a4781",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Run the labs\n",
    "\n",
    "You can run the labs locally on your machine or you can use cloud environments provided by Google Colab or Binderhub. **If you're working with Google Colab and Binderhub be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.**\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/data-analysis-3300-3003/colab/blob/main/lab-3-eda.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "<a href=\"https://mybinder.org/v2/gh/binder-3300-3003/binder/HEAD\" target=\"_blank\">\n",
    "  <img src=\"https://mybinder.org/badge_logo.svg\" alt=\"Open In Binder\"/>\n",
    "</a>\n",
    "\n",
    "### Download data\n",
    "\n",
    "If you need to download the data for this lab, run the following code snippet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0721da-91c0-485f-8c76-1fe26cc91e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"week-3\" not in os.listdir(os.getcwd()):\n",
    "    os.system('wget \"https://github.com/data-analysis-3300-3003/data/raw/main/data/week-3.zip\"')\n",
    "    os.system('unzip \"week-3.zip\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a9f398-37ef-42cf-a344-aee4f8eabde4",
   "metadata": {},
   "source": [
    "### Working in Colab\n",
    "\n",
    "If you're working in Google Colab, you'll need to run the following code snippet to install the required packages that don't come with the colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be90d5ea-91bc-4fe9-a396-00d4e4e10711",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopandas\n",
    "!pip install pyarrow\n",
    "!pip install mapclassify\n",
    "!pip install rasterio\n",
    "!pip install libpysal\n",
    "!pip install esda\n",
    "!pip install splot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a91b416-8469-4a0c-938a-160ea1cdfe2c",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a992cb-a166-441d-a313-4e218396a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import plotly.io as pio\n",
    "import esda\n",
    "\n",
    "from splot.esda import plot_moran\n",
    "from libpysal.weights import KNN, lag_spatial\n",
    "pio.renderers.default = \"colab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb674bae-07ca-420e-b6c1-7109cf092fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the crop yield data\n",
    "crop_yield_data_path = os.path.join(os.getcwd(), \"week-3\")\n",
    "\n",
    "# Get a list of crop yield data\n",
    "crop_yield_data_files = os.listdir(crop_yield_data_path)\n",
    "\n",
    "# Combine the geojson files into one GeoDataFrame\n",
    "dfs = []\n",
    "\n",
    "for i in crop_yield_data_files:\n",
    "    if i.endswith(\".geojson\"):\n",
    "        print(f\"Loading file {i} into a Geopandas GeoDataFrame\")\n",
    "        tmp_df = gpd.read_file(os.path.join(crop_yield_data_path, i))\n",
    "        dfs.append(tmp_df)\n",
    "\n",
    "gdf = pd.concat(dfs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2de0116-5d5f-4e58-ad18-ad9063f1512b",
   "metadata": {},
   "source": [
    "## Data summaries\n",
    "\n",
    "An initial data exploration task is to produce summary statistics for the variables in our datasets. Pandas `DataFrame`s and GeoPandas `GeoDataFrame`s have a `describe()` method which generates a `DataFrame` of summary statistics for each variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66aecec-9ae1-458a-91b1-697199352fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe our DataFrame of crop yield data\n",
    "gdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c8c036-a285-4d95-9ff2-6d95cc86c5a0",
   "metadata": {},
   "source": [
    "`describe()` returns to us a count of the number of observations in variable, the mean value for observations in a variable, and summary statistics describing the distribution and range of values (standard deviation, percentiles (median = 50th percentile), and min and max values).\n",
    "\n",
    "However, there are two main groups in our dataset: canola observations and wheat observations denoted by the `Variety` column. It will be more informative to generate summary statistcs for each group separately. We can do this using the Pandas `groupby()` function which splits an `DataFrame` into subsets based upon a grouping variable, computes statistics for each subset, and then combines the results. Here, we need to `groupby()` `Variety` to generate summary statistics for each crop type. \n",
    "\n",
    "We'll also generate these summary statistics within a context manager (denoted by a `with` block). This context allows us to change the default display values for a `DataFrame` only for this context without affecting the global defaults that apply to the rest of the notebook. This is a useful trick in case you have a particular need to control how a `DataFrame` is displayed (e.g. printing all rows as specified by the `display.max_rows` option)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8907c60d-4f8f-4407-a132-d6404f446bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.float_format', lambda x: '%.3f' % x):\n",
    "    display(gdf.groupby([\"Variety\"]).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f52aa7-b6a8-4a22-aac8-3d297ce9a610",
   "metadata": {},
   "source": [
    "This still isn't a very helpful layout to view the summary statistics as not all of the statistics can be displayed. Let's transpose the summary statistics so rows become columns and vice versa using the `T` transpose operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c859a2-5d02-4c1f-8514-694838b98488",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.float_format', lambda x: '%.3f' % x):\n",
    "    display(gdf.loc[:, [\"Variety\", \"DryYield\", \"gndvi\", \"ndyi\"]].groupby([\"Variety\"]).describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc6e979-8c49-41c8-bbad-7813c0b22391",
   "metadata": {},
   "source": [
    "## Data distributions\n",
    "\n",
    "The mean tells us the average value for a variable. However, it is susceptible to outliers and extreme values. Therefore, it is important to view the mean and the median (50th percentile) together as the median is not affected by extreme values. \n",
    "\n",
    "However, neither the mean or the median reveal the spread or distribution of values for a variable. The min and max values tell us what the range of values are for a variable. This can be useful for detecting potential measurement error and noise (e.g. is the max value for wheat yield sensible?). However, the min and max values can be affected by extreme values and don't tell us anything about the shape or density of the distribution of data values. \n",
    "\n",
    "The inter-quartile range (difference between the 75th and 25th percentile values) tells us how spread out the data is around the median and the standard deviation tells us how spread out the data is around the mean. Assuming a normal distribution, ~68% of the values are within one standard deviation of the mean. Thus, if the standard deviation is small relative to the mean it indicates there is not much spread in the data away from the mean.\n",
    "\n",
    "It is often useful to visualise the distribution of variables. A histogram is a common visualisation for distributions. The height of the bars of a histogram correspond to the count of values that fall within the bin. The width of the bar corresponds to the bin width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b92b3c3-5505-4bb2-b319-53d38a93332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data_frame=gdf, \n",
    "    x=\"DryYield\", \n",
    "    facet_col=\"Variety\", \n",
    "    hover_data=[\"DryYield\", \"Elevation\", \"WetMass\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9374d-2deb-4af9-9a21-b17714c97080",
   "metadata": {},
   "source": [
    "The choice of bin width will affect the histogram. Too small a bin width will lead to small peaks in the distribution of data values being visualised which can obscure the dominant pattern in the data. Conversely, too large a bin width could mask important parts of a variable's distribution. The `histogram()` function from Plotly Express has a `nbins` parameter that can be used to specify the number of bins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085e8ed0-6b7a-470c-a46f-797835952c29",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "The majority of data values on the histograms above are concentrated on the far left of the figure. If you zoom in you will see there are a few isolated extreme or outlier yield values, which are masking the dominant pattern of the distribution. Detecting outliers is an important part of exploratory data analysis. \n",
    "\n",
    "Now that outliers have been detected we need to fix or remove them. A common way to detect outliers is to use a threshold based on percentile or standard deviation values. Here, we'll say an outlier is any value that is more or less than three standard deviations from the mean. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f881c92a-2e70-4d4c-aa36-84bb3090d2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canola\n",
    "df_canola = gdf.loc[gdf[\"Variety\"] == \"43Y23 RR\", :]\n",
    "print(f\"There are {df_canola.shape[0]} canola rows BEFORE dropping outliers\")\n",
    "df_canola = df_canola.loc[(df_canola[\"DryYield\"]-df_canola[\"DryYield\"].mean()).abs() < (3*df_canola[\"DryYield\"].std()), :]\n",
    "print(f\"There are {df_canola.shape[0]} canola rows AFTER dropping outliers\")\n",
    "\n",
    "# Wheat\n",
    "df_wheat = gdf.loc[gdf[\"Variety\"] == \"Ninja\", :]\n",
    "print(f\"There are {df_wheat.shape[0]} wheat rows BEFORE dropping outliers\")\n",
    "df_wheat = df_wheat.loc[(df_wheat[\"DryYield\"]-df_wheat[\"DryYield\"].mean()).abs() < (3*df_wheat[\"DryYield\"].std()), :]\n",
    "print(f\"There are {df_wheat.shape[0]} wheat rows AFTER dropping outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f01df53-89e0-462a-9ecf-336ec00551c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine filtered dfs\n",
    "gdf_clean = pd.concat([df_canola, df_wheat], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6158e-05e4-4a07-bdd2-ce46832409e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data_frame=gdf_clean, \n",
    "    x=\"DryYield\", \n",
    "    facet_col=\"Variety\", \n",
    "    marginal=\"box\", \n",
    "    hover_data=[\"DryYield\", \"Elevation\", \"WetMass\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f3baca-6172-416e-bde5-5db85fcf13e0",
   "metadata": {},
   "source": [
    "Instead of dropping rows where there are extreme crop yield values, we can also replace outlier values with a more sensible value such as the mean.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc441774-30f0-4d74-92ee-d4ecdda714f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_yield = gdf.loc[:, [\"Variety\", \"DryYield\"]].groupby([\"Variety\"]).mean()\n",
    "mean_yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d584f0-f3f1-4066-8fb3-6697fdf73bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_canola = gdf.loc[gdf[\"Variety\"] == \"43Y23 RR\", :]\n",
    "df_canola.loc[(df_canola[\"DryYield\"]-df_canola[\"DryYield\"].mean()).abs() > (3*df_canola[\"DryYield\"].std()), \"DryYield\"] = mean_yield.iloc[0, 0]\n",
    "\n",
    "df_wheat = gdf.loc[gdf[\"Variety\"] == \"Ninja\", :]\n",
    "df_wheat.loc[(df_wheat[\"DryYield\"]-df_wheat[\"DryYield\"].mean()).abs() > (3*df_wheat[\"DryYield\"].std()), \"DryYield\"] = mean_yield.iloc[1, 0]\n",
    "\n",
    "# combine filtered dfs\n",
    "gdf_replaced = pd.concat([df_canola, df_wheat], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211f5b0-246f-4abe-9383-5cca6a91658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data_frame=gdf_replaced, \n",
    "    x=\"DryYield\", \n",
    "    facet_col=\"Variety\", \n",
    "    marginal=\"box\", \n",
    "    hover_data=[\"DryYield\", \"Elevation\", \"WetMass\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53126590-bf57-4d24-a0b4-4edb8424e347",
   "metadata": {},
   "source": [
    "Looking at the wheat yield histogram we can see there are a large number of zero or close to zero values. This is another strange artefact in the distribution of our data values. Are zero crop yield values actually no crop yield from the plant or a source of measurement error or other noise? If the latter, we should remove these noisy values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b0bed-b7dc-4bbe-9000-68042b9140b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_canola = gdf_clean.loc[gdf_clean[\"Variety\"] == \"43Y23 RR\", :]\n",
    "print(f\"The number of canola observations with yield values of zero or less is: {(df_canola['DryYield'] <= 0).sum()}\")\n",
    "df_wheat = gdf_clean.loc[gdf_clean[\"Variety\"] == \"Ninja\", :]\n",
    "print(f\"The number of wheat observations with yield values of zero or less is: {(df_wheat['DryYield'] <= 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a129187-56a8-4d9a-8163-387c60f09b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_canola.loc[df_canola[\"DryYield\"] <= 0, \"DryYield\"] = np.nan\n",
    "df_wheat.loc[df_wheat[\"DryYield\"] <= 0, \"DryYield\"] = np.nan\n",
    "\n",
    "# combine filtered dfs\n",
    "gdf_with_nan = pd.concat([df_canola, df_wheat], axis=0)\n",
    "\n",
    "# drop NAs\n",
    "gdf_dropped_nan = gdf_with_nan.dropna(subset=[\"DryYield\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e52a86-3eb9-4530-9c32-bd50e2e4fa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data_frame=gdf_dropped_nan, \n",
    "    x=\"DryYield\", \n",
    "    facet_col=\"Variety\", \n",
    "    marginal=\"box\", \n",
    "    hover_data=[\"DryYield\", \"Elevation\", \"WetMass\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b55652-a1d9-411c-83b4-8c9d19f791bf",
   "metadata": {},
   "source": [
    "Now we've removed zero values the distribution of our crop yield values looks more sensible and relatively normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548521f8-21e1-42e1-aa73-0e34babc6f8b",
   "metadata": {},
   "source": [
    "### 2D histograms\n",
    "\n",
    "We can use 2D histograms or density heatmaps to look at the distribution of two variables together. 2D hisograms are a useful complement to scatter plots when you have a large number of observations. Here, colour is used to represent the distribution of data values as opposed to the height of rectangular bars on a histogram.\n",
    "\n",
    "Let's create 2D histograms to visualise the relationship between vegetation indices and canola crop yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621215a-4996-432a-8a60-c5dc72ff4b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.density_heatmap(\n",
    "    data_frame=gdf_dropped_nan.loc[gdf_dropped_nan[\"Variety\"] == \"43Y23 RR\", :], \n",
    "    x=\"DryYield\", \n",
    "    y=\"gndvi\", \n",
    "    marginal_x=\"box\", \n",
    "    marginal_y=\"box\",\n",
    "    range_y=[0.4, 0.8])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24624ba2-fc09-4a99-bf2d-3b0925913ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.density_heatmap(\n",
    "    data_frame=gdf_dropped_nan.loc[gdf_dropped_nan[\"Variety\"] == \"43Y23 RR\", :], \n",
    "    x=\"DryYield\", \n",
    "    y=\"ndyi\", \n",
    "    marginal_x=\"box\", \n",
    "    marginal_y=\"box\",\n",
    "    range_y=[0.1, 0.5])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077f4f8d-df69-46b4-b38f-ba7bb66ca259",
   "metadata": {},
   "source": [
    "### Violin plots\n",
    "\n",
    "One of the limits of using histograms to visualise distributions is the size of the bins affects the distribution of data values. An alternative approach to visualising a distribution is to use a violin plot. \n",
    "\n",
    "Violin plots display use density and box plots to visualise distributions, which look similar to violins. The density is the probability of an observation taking on a certain value and is plotted as a smooth curve. Areas where the curve is fatter indicate a higher probability that an observation will take that value. Box plots display the 25th, 50th, and 75th percentile values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65946cff-9ec5-4245-ba54-b31df6582d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.violin(\n",
    "    gdf_dropped_nan, \n",
    "    y=\"DryYield\", \n",
    "    x=\"Variety\", \n",
    "    color=\"Variety\", \n",
    "    box=True, \n",
    "    points=\"outliers\", \n",
    "    hover_data=[\"DryYield\", \"gndvi\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bf4951-5ce3-46f8-a981-bb075b3f3510",
   "metadata": {},
   "source": [
    "## Covariation and correlation\n",
    "\n",
    "Variation describes the distribution of values within a variable, covariation describes how values vary between two variables. Postive covariance indicates that as the values of one variable increase so do the values of the other variable (or they both decrease together). Negative covariance indicates that as values of one variable increase the values of the other variable decrease. \n",
    "\n",
    "We can use scatter plots to explore the covariance in our data. We did this above when we looked at the relationships between NDYI and GNDVI and crop yield.\n",
    "\n",
    "We can compute the covariance between two variables as:\n",
    "\n",
    "$$Cov(X,Y)=\\frac{1}{N}\\sum_{i=1}^{n}(X_{i}-\\bar{X})(Y_{i}-\\bar{Y})$$\n",
    "\n",
    "However, a limit of using the covariance to measure association and relationships between variables is that it is affected by the units of measurement. For example, if we had the same crop yield measurements but in units of tonnes per hectare of kilograms per hectare we'd get different covariance scores. Therefore, the correlation coefficient is often used as a measure of association between variables. \n",
    "\n",
    "$$Corr{X,Y} = \\frac{Cov(X,Y)}{sd(X) \\cdot sd(Y)} = \\frac{\\sigma_{XY}}{\\sigma{X}\\sigma{Y}}$$\n",
    "\n",
    "The correlation coefficient is bound between -1 and 1 with -1 indicating perfect negative correlation and 1 indicating perfect positive correlation. \n",
    "\n",
    "Pandas has a <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html\" target=\"_blank\">`corr`</a> function that can be used to compute correlation coefficients between columns in a `DataFrame`. Let's compute the correlation coefficients between crop yields and NDYI and GNDVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e72820e-f42b-4198-becc-b1af3f0963dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_corr = gdf_dropped_nan.loc[:, [\"Variety\", \"DryYield\", \"gndvi\", \"ndyi\"]].groupby([\"Variety\"]).corr()\n",
    "gdf_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badc9db6-1add-41b7-a389-7ef02099c776",
   "metadata": {},
   "source": [
    "## Spatial correlation\n",
    "\n",
    "Spatial correlation or spatial autocorrelation describes how similar values are for observations that are close to each other in space. Spatial autocorrelation is the degree to which similar values cluster together in space and is the \"absence of spatial randomness\" <a href=\"https://geographicdata.science/book/notebooks/06_spatial_autocorrelation.html\" target=\"_blank\">(Rey et al. 2020)</a>. Spatial randomness occurs when the values of observations display have no relationship with their location in space.\n",
    "\n",
    "Some key terms related to spatial autocorrelation:\n",
    "\n",
    "* *positive spatial autocorrelation*: the values of nearby locations are similar (e.g. a location and its neighbours both have high or low values).\n",
    "* *negative spatial autocorrelation*: similar values are located far way from each other (less common than spatial autocorrelation). \n",
    "* *global spatial autocorrelation*: a description or summary of the general trend of spatial autocorrelation in the dataset.\n",
    "* *local spatial autocorrelation*: a local measure of how similar an observation's values are to its neighbours which is useful for identifying clusters or similar or disimilar values in a dataset.\n",
    "\n",
    "We can visually explore the spatial correlation in our data by plotting it on a map and using a suitable colour scale to map data values to colours. Let's visualise the canola crop yield values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb453866-6ac2-4541-8948-b306d1c910bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_canola = gdf_dropped_nan.loc[gdf_dropped_nan[\"Variety\"] == \"43Y23 RR\", :].copy()\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "    df_canola, \n",
    "    lat=df_canola.geometry.y,\n",
    "    lon=df_canola.geometry.x,\n",
    "    color=\"DryYield\", \n",
    "    mapbox_style=\"open-street-map\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90e3b3-b0e2-4e0b-a35e-cb5ab0c7e7d1",
   "metadata": {},
   "source": [
    "Visually, we can see some spatial patterns in our canola yield data. High yield values with yellow shades appear to be clustered together in the field. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42efef4-df2e-42bb-a954-b1625cc20bc7",
   "metadata": {},
   "source": [
    "While we can get a general sense of the degreen of spatial autocorrelation in our dataset by viewing the data on a map, this does not provide us with a formal method to quantify how spatially correlated the dataset is or how likely this level of spatial correlation is under a context of spatial randomness. \n",
    "\n",
    "The global Moran's I statistic can be used as a measure of spatial autocorrelation in the dataset and for undertaking statistical inference to assess how likely realising the observed pattern of spatial correlation is under a context of spatial randomness. This is analogous to statistical significance / hypothesis testing. \n",
    "\n",
    "The global Moran's I statistic is a measure the correlation between an observations values and its neighbours. The value of a neighbouring values is termed the spatial lag.\n",
    "\n",
    "A scatter plot can help us build up our intuition of the Moran's I statistic. First, let's compute each canola yield observation's spatial lag (i.e. the average crop yield of it's four nearest neighbours here - there a range of methods for defining neighbouring values).\n",
    "\n",
    "We do this by generating a spatial weights matrix. This is an object that keeps a record of the which data points are neighbours of a given observation in our `GeoDataFrame`. Here, we're defining neighbours using the K nearest neighbour rule; specifically the four nearest neighbours. \n",
    "\n",
    "Here, we row standardise the weights in the spatial weights matrix so each neighbour contributes equal weight and the sum of the neighbour's weights is one. This has the useful effect of the sum of each neighbours weight multiplied by its value returning the average value for all neighbours of an observation: its spatial lag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8cca34-4118-4eeb-93a2-c402d49136d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spatial weights matrix to identify a locations neighbouring values\n",
    "w_knn = KNN.from_dataframe(df_canola, k=4)\n",
    "# Row-standardization\n",
    "w_knn.transform = \"R\"\n",
    "print(f\"the neighbours of the first observation in our dataset are {w_knn.neighbors[0]}\")\n",
    "\n",
    "# compute spatial lag of canola yield\n",
    "df_canola[\"DryYield_SpatialLag\"] = lag_spatial(w_knn, df_canola[\"DryYield\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7573376-177a-487a-8d36-c48fe102b184",
   "metadata": {},
   "source": [
    "The Moran's plot is a scatter plot that visualises the variable of interest, crop yield here, against its spatial lag. This scatter plot reveals the pattern of spatial autocorrelation in the dataset. If we see a trend of values from the bottom left to top right corners of the graph it indicates there is positive spatial autocorrelation, a trend of values from top left to bottom right indicates negative spatial autocorrelation, and no visible trend indicates spatial randomness. \n",
    "\n",
    "The Moran's plot uses standardised values where the global mean for a variable is subtracted from each observation. \n",
    "\n",
    "The slope of the trend line on the scatter plot is the Moran's I statistic, it indicates the sign and strength of spatial correlation in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92ce0de-9c96-44d0-b198-833671753d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise yield measurements\n",
    "df_canola[\"DryYield_std\"] = df_canola[\"DryYield\"] - df_canola[\"DryYield\"].mean()\n",
    "df_canola[\"DryYield_SpatialLag_std\"] = df_canola[\"DryYield_SpatialLag\"] - df_canola[\"DryYield_SpatialLag\"].mean()\n",
    "\n",
    "# morans plot\n",
    "fig = px.scatter(\n",
    "    df_canola,\n",
    "    x = \"DryYield_std\",\n",
    "    y = \"DryYield_SpatialLag_std\",\n",
    "    trendline = \"ols\",\n",
    "    opacity=0.05,\n",
    "    labels={\"DryYield_std\": \"Standardised Canola Yield\",\n",
    "           \"DryYield_SpatialLag_std\": \"Standardised Canola Yield (spatial lag)\"}\n",
    ")\n",
    "fig.add_hline(y=0, line_width=0.5)\n",
    "fig.add_vline(x=0, line_width=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6905ed-b2c2-4b16-823f-79114fd9dc36",
   "metadata": {},
   "source": [
    "We can also compute the Moran's I statistic which is a statistical summary of the general level of spatial autocorrelation in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffaed02-14e2-4639-8d1f-e98ac2f6fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "moran = esda.moran.Moran(df_canola[\"DryYield\"], w_knn)\n",
    "print(f\"Moran's I statistic is: {round(moran.I, 3)}\")\n",
    "print(f\"P-value for Moran's I statistic is: {moran.p_sim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4a592-0362-48d7-9efa-b876a93c38ba",
   "metadata": {},
   "source": [
    "This p-value indicates the probability of obtaining a Moran's I statistic this large over 999 simulations with spatial randomness. This indicates that we can reject a null hypothesis of canola yield values being arranged in a spatially random manner in the field. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
